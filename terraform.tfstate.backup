{
  "version": 4,
  "terraform_version": "0.14.10",
  "serial": 51,
  "lineage": "bf8c7058-6869-65c6-702f-33ae5aab7553",
  "outputs": {},
  "resources": [
    {
      "mode": "data",
      "type": "template_file",
      "name": "prometheus_values",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "b4df124e8c9a0326756fac307a5e299da5ccae72df89b3aa531a195a34a4d553",
            "rendered": "rbac:\n  create: true\n\npodSecurityPolicy:\n  enabled: false\n\nimagePullSecrets:\n# - name: \"image-pull-secret\"\n\n## Define serviceAccount names for components. Defaults to component's fully qualified name.\n##\nserviceAccounts:\n  alertmanager:\n    create: true\n    name:\n    annotations: {}\n  nodeExporter:\n    create: true\n    name:\n    annotations: {}\n  pushgateway:\n    create: true\n    name:\n    annotations: {}\n  server:\n    create: true\n    name:\n    annotations: {}\n\nalertmanager:\n  ## If false, alertmanager will not be installed\n  ##\n  enabled: true\n\n  ## Use a ClusterRole (and ClusterRoleBinding)\n  ## - If set to false - we define a Role and RoleBinding in the defined namespaces ONLY\n  ## This makes alertmanager work - for users who do not have ClusterAdmin privs, but wants alertmanager to operate on their own namespaces, instead of clusterwide.\n  useClusterRole: true\n\n  ## Set to a rolename to use existing role - skipping role creating - but still doing serviceaccount and rolebinding to the rolename set here.\n  useExistingRole: false\n\n  ## alertmanager container name\n  ##\n  name: alertmanager\n\n  ## alertmanager container image\n  ##\n  image:\n    repository: quay.io/prometheus/alertmanager\n    tag: v0.23.0\n    pullPolicy: IfNotPresent\n\n  ## alertmanager priorityClassName\n  ##\n  priorityClassName: \"\"\n\n  ## Additional alertmanager container arguments\n  ##\n  extraArgs: {}\n\n  ## Additional InitContainers to initialize the pod\n  ##\n  extraInitContainers: []\n\n  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug\n  ## so that the various internal URLs are still able to access as they are in the default case.\n  ## (Optional)\n  prefixURL: \"\"\n\n  ## External URL which can access alertmanager\n  baseURL: \"http://localhost:9093\"\n\n  ## Additional alertmanager container environment variable\n  ## For instance to add a http_proxy\n  ##\n  extraEnv: {}\n\n  ## Additional alertmanager Secret mounts\n  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.\n  extraSecretMounts: []\n    # - name: secret-files\n    #   mountPath: /etc/secrets\n    #   subPath: \"\"\n    #   secretName: alertmanager-secret-files\n    #   readOnly: true\n\n  ## Additional alertmanager Configmap mounts\n  extraConfigmapMounts: []\n    # - name: template-files\n    #   mountPath: /etc/config/templates.d\n    #   configMap: alertmanager-template-files\n    #   readOnly: true\n\n  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.alertmanager.configMapOverrideName}}\n  ## Defining configMapOverrideName will cause templates/alertmanager-configmap.yaml\n  ## to NOT generate a ConfigMap resource\n  ##\n  configMapOverrideName: \"\"\n\n  ## The name of a secret in the same kubernetes namespace which contains the Alertmanager config\n  ## Defining configFromSecret will cause templates/alertmanager-configmap.yaml\n  ## to NOT generate a ConfigMap resource\n  ##\n  configFromSecret: \"\"\n\n  ## The configuration file name to be loaded to alertmanager\n  ## Must match the key within configuration loaded from ConfigMap/Secret\n  ##\n  configFileName: alertmanager.yml\n\n  ingress:\n    ## If true, alertmanager Ingress will be created\n    ##\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    ## alertmanager Ingress annotations\n    ##\n    annotations: {}\n    #   kubernetes.io/ingress.class: nginx\n    #   kubernetes.io/tls-acme: 'true'\n\n    ## alertmanager Ingress additional labels\n    ##\n    extraLabels: {}\n\n    ## alertmanager Ingress hostnames with optional path\n    ## Must be provided if Ingress is enabled\n    ##\n    hosts: []\n    #   - alertmanager.domain.com\n    #   - domain.com/alertmanager\n\n    path: /\n\n    # pathType is only for k8s \u003e= 1.18\n    pathType: Prefix\n\n    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.\n    extraPaths: []\n    # - path: /*\n    #   backend:\n    #     serviceName: ssl-redirect\n    #     servicePort: use-annotation\n\n    ## alertmanager Ingress TLS configuration\n    ## Secrets must be manually created in the namespace\n    ##\n    tls: []\n    #   - secretName: prometheus-alerts-tls\n    #     hosts:\n    #       - alertmanager.domain.com\n\n  ## Alertmanager Deployment Strategy type\n  # strategy:\n  #   type: Recreate\n\n  ## Node tolerations for alertmanager scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal|Exists\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  ## Node labels for alertmanager pod assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Pod affinity\n  ##\n  affinity: {}\n\n  ## PodDisruptionBudget settings\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n  ##\n  podDisruptionBudget:\n    enabled: false\n    maxUnavailable: 1\n\n  ## Use an alternate scheduler, e.g. \"stork\".\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  # schedulerName:\n\n  persistentVolume:\n    ## If true, alertmanager will create/use a Persistent Volume Claim\n    ## If false, use emptyDir\n    ##\n    enabled: true\n\n    ## alertmanager data Persistent Volume access modes\n    ## Must match those of existing PV or dynamic provisioner\n    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    ##\n    accessModes:\n      - ReadWriteOnce\n\n    ## alertmanager data Persistent Volume Claim annotations\n    ##\n    annotations: {}\n\n    ## alertmanager data Persistent Volume existing claim name\n    ## Requires alertmanager.persistentVolume.enabled: true\n    ## If defined, PVC must be created manually before volume will be bound\n    existingClaim: \"\"\n\n    ## alertmanager data Persistent Volume mount root path\n    ##\n    mountPath: /data\n\n    ## alertmanager data Persistent Volume size\n    ##\n    size: 2Gi\n\n    ## alertmanager data Persistent Volume Storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    # storageClass: \"-\"\n\n    ## alertmanager data Persistent Volume Binding Mode\n    ## If defined, volumeBindingMode: \u003cvolumeBindingMode\u003e\n    ## If undefined (the default) or set to null, no volumeBindingMode spec is\n    ##   set, choosing the default mode.\n    ##\n    # volumeBindingMode: \"\"\n\n    ## Subdirectory of alertmanager data Persistent Volume to mount\n    ## Useful if the volume's root directory is not empty\n    ##\n    subPath: \"\"\n\n    ## Persistent Volume Claim Selector\n    ## Useful if Persistent Volumes have been provisioned in advance\n    ## Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector\n    ##\n    # selector:\n    #  matchLabels:\n    #    release: \"stable\"\n    #  matchExpressions:\n    #    - { key: environment, operator: In, values: [ dev ] }\n\n  emptyDir:\n    ## alertmanager emptyDir volume size limit\n    ##\n    sizeLimit: \"\"\n\n  ## Annotations to be added to alertmanager pods\n  ##\n  podAnnotations: {}\n    ## Tell prometheus to use a specific set of alertmanager pods\n    ## instead of all alertmanager pods found in the same namespace\n    ## Useful if you deploy multiple releases within the same namespace\n    ##\n    ## prometheus.io/probe: alertmanager-teamA\n\n  ## Labels to be added to Prometheus AlertManager pods\n  ##\n  podLabels: {}\n\n  ## Specify if a Pod Security Policy for node-exporter must be created\n  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  ##\n  podSecurityPolicy:\n    annotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)\n  ##\n  replicaCount: 1\n\n  ## Annotations to be added to deployment\n  ##\n  deploymentAnnotations: {}\n\n  statefulSet:\n    ## If true, use a statefulset instead of a deployment for pod management.\n    ## This allows to scale replicas to more than 1 pod\n    ##\n    enabled: false\n\n    annotations: {}\n    labels: {}\n    podManagementPolicy: OrderedReady\n\n    ## Alertmanager headless service to use for the statefulset\n    ##\n    headless:\n      annotations: {}\n      labels: {}\n\n      ## Enabling peer mesh service end points for enabling the HA alert manager\n      ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md\n      enableMeshPeer: false\n\n      servicePort: 80\n\n  ## alertmanager resource requests and limits\n  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n    # limits:\n    #   cpu: 10m\n    #   memory: 32Mi\n    # requests:\n    #   cpu: 10m\n    #   memory: 32Mi\n\n  # Custom DNS configuration to be added to alertmanager pods\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n\n  ## Security context to be added to alertmanager pods\n  ##\n  securityContext:\n    runAsUser: 65534\n    runAsNonRoot: true\n    runAsGroup: 65534\n    fsGroup: 65534\n\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n    ## Enabling peer mesh service end points for enabling the HA alert manager\n    ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md\n    # enableMeshPeer : true\n\n    ## List of IP addresses at which the alertmanager service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 80\n    # nodePort: 30000\n    sessionAffinity: None\n    type: ClusterIP\n\n  ## List of initial peers\n  ## Ref: https://github.com/prometheus/alertmanager/blob/main/README.md#high-availability\n  clusterPeers: []\n\n## Monitors ConfigMap changes and POSTs to a URL\n## Ref: https://github.com/jimmidyson/configmap-reload\n##\nconfigmapReload:\n  prometheus:\n    ## If false, the configmap-reload container will not be deployed\n    ##\n    enabled: true\n\n    ## configmap-reload container name\n    ##\n    name: configmap-reload\n\n    ## configmap-reload container image\n    ##\n    image:\n      repository: jimmidyson/configmap-reload\n      tag: v0.5.0\n      pullPolicy: IfNotPresent\n\n    ## Additional configmap-reload container arguments\n    ##\n    extraArgs: {}\n    ## Additional configmap-reload volume directories\n    ##\n    extraVolumeDirs: []\n\n\n    ## Additional configmap-reload mounts\n    ##\n    extraConfigmapMounts: []\n      # - name: prometheus-alerts\n      #   mountPath: /etc/alerts.d\n      #   subPath: \"\"\n      #   configMap: prometheus-alerts\n      #   readOnly: true\n\n\n    ## configmap-reload resource requests and limits\n    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n  alertmanager:\n    ## If false, the configmap-reload container will not be deployed\n    ##\n    enabled: true\n\n    ## configmap-reload container name\n    ##\n    name: configmap-reload\n\n    ## configmap-reload container image\n    ##\n    image:\n      repository: jimmidyson/configmap-reload\n      tag: v0.5.0\n      pullPolicy: IfNotPresent\n\n    ## Additional configmap-reload container arguments\n    ##\n    extraArgs: {}\n    ## Additional configmap-reload volume directories\n    ##\n    extraVolumeDirs: []\n\n\n    ## Additional configmap-reload mounts\n    ##\n    extraConfigmapMounts: []\n      # - name: prometheus-alerts\n      #   mountPath: /etc/alerts.d\n      #   subPath: \"\"\n      #   configMap: prometheus-alerts\n      #   readOnly: true\n\n\n    ## configmap-reload resource requests and limits\n    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n\nkubeStateMetrics:\n  ## If false, kube-state-metrics sub-chart will not be installed\n  ##\n  enabled: true\n\n## kube-state-metrics sub-chart configurable values\n## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics\n##\n# kube-state-metrics:\n\nnodeExporter:\n  ## If false, node-exporter will not be installed\n  ##\n  enabled: true\n\n  ## If true, node-exporter pods share the host network namespace\n  ##\n  hostNetwork: true\n\n  ## If true, node-exporter pods share the host PID namespace\n  ##\n  hostPID: true\n\n  ## If true, node-exporter pods mounts host / at /host/root\n  ##\n  hostRootfs: true\n\n  ## node-exporter container name\n  ##\n  name: node-exporter\n\n  ## node-exporter container image\n  ##\n  image:\n    repository: quay.io/prometheus/node-exporter\n    tag: v1.3.0\n    pullPolicy: IfNotPresent\n\n  ## Specify if a Pod Security Policy for node-exporter must be created\n  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  ##\n  podSecurityPolicy:\n    annotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## node-exporter priorityClassName\n  ##\n  priorityClassName: \"\"\n\n  ## Custom Update Strategy\n  ##\n  updateStrategy:\n    type: RollingUpdate\n\n  ## Additional node-exporter container arguments\n  ##\n  extraArgs: {}\n\n  ## Additional InitContainers to initialize the pod\n  ##\n  extraInitContainers: []\n\n  ## Additional node-exporter hostPath mounts\n  ##\n  extraHostPathMounts: []\n    # - name: textfile-dir\n    #   mountPath: /srv/txt_collector\n    #   hostPath: /var/lib/node-exporter\n    #   readOnly: true\n    #   mountPropagation: HostToContainer\n\n  extraConfigmapMounts: []\n    # - name: certs-configmap\n    #   mountPath: /prometheus\n    #   configMap: certs-configmap\n    #   readOnly: true\n\n  ## Node tolerations for node-exporter scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal|Exists\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  ## Node labels for node-exporter pod assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Annotations to be added to node-exporter pods\n  ##\n  podAnnotations: {}\n\n  ## Labels to be added to node-exporter pods\n  ##\n  pod:\n    labels: {}\n\n  ## PodDisruptionBudget settings\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n  ##\n  podDisruptionBudget:\n    enabled: false\n    maxUnavailable: 1\n\n  ## node-exporter resource limits \u0026 requests\n  ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n    # limits:\n    #   cpu: 200m\n    #   memory: 50Mi\n    # requests:\n    #   cpu: 100m\n    #   memory: 30Mi\n\n  # Custom DNS configuration to be added to node-exporter pods\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n\n  ## Security context to be added to node-exporter pods\n  ##\n  securityContext:\n    fsGroup: 65534\n    runAsGroup: 65534\n    runAsNonRoot: true\n    runAsUser: 65534\n\n  service:\n    annotations:\n      prometheus.io/scrape: \"true\"\n    labels: {}\n\n    # Exposed as a headless service:\n    # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services\n    clusterIP: None\n\n    ## List of IP addresses at which the node-exporter service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    hostPort: 9100\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 9100\n    type: ClusterIP\n\nserver:\n  ## Prometheus server container name\n  ##\n  enabled: true\n\n  ## Use a ClusterRole (and ClusterRoleBinding)\n  ## - If set to false - we define a RoleBinding in the defined namespaces ONLY\n  ##\n  ## NB: because we need a Role with nonResourceURL's (\"/metrics\") - you must get someone with Cluster-admin privileges to define this role for you, before running with this setting enabled.\n  ##     This makes prometheus work - for users who do not have ClusterAdmin privs, but wants prometheus to operate on their own namespaces, instead of clusterwide.\n  ##\n  ## You MUST also set namespaces to the ones you have access to and want monitored by Prometheus.\n  ##\n  # useExistingClusterRoleName: nameofclusterrole\n\n  ## namespaces to monitor (instead of monitoring all - clusterwide). Needed if you want to run without Cluster-admin privileges.\n  # namespaces:\n  #   - yournamespace\n\n  name: server\n\n  # sidecarContainers - add more containers to prometheus server\n  # Key/Value where Key is the sidecar `- name: \u003cKey\u003e`\n  # Example:\n  #   sidecarContainers:\n  #      webserver:\n  #        image: nginx\n  sidecarContainers: {}\n\n  # sidecarTemplateValues - context to be used in template for sidecarContainers\n  # Example:\n  #   sidecarTemplateValues: *your-custom-globals\n  #   sidecarContainers:\n  #     webserver: |-\n  #       {{ include \"webserver-container-template\" . }}\n  # Template for `webserver-container-template` might looks like this:\n  #   image: \"{{ .Values.server.sidecarTemplateValues.repository }}:{{ .Values.server.sidecarTemplateValues.tag }}\"\n  #   ...\n  #\n  sidecarTemplateValues: {}\n\n  ## Prometheus server container image\n  ##\n  image:\n    repository: quay.io/prometheus/prometheus\n    tag: v2.31.1\n    pullPolicy: IfNotPresent\n\n  ## prometheus server priorityClassName\n  ##\n  priorityClassName: \"\"\n\n  ## EnableServiceLinks indicates whether information about services should be injected\n  ## into pod's environment variables, matching the syntax of Docker links.\n  ## WARNING: the field is unsupported and will be skipped in K8s prior to v1.13.0.\n  ##\n  enableServiceLinks: true\n\n  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug\n  ## so that the various internal URLs are still able to access as they are in the default case.\n  ## (Optional)\n  prefixURL: \"\"\n\n  ## External URL which can access prometheus\n  ## Maybe same with Ingress host name\n  baseURL: \"\"\n\n  ## Additional server container environment variables\n  ##\n  ## You specify this manually like you would a raw deployment manifest.\n  ## This means you can bind in environment variables from secrets.\n  ##\n  ## e.g. static environment variable:\n  ##  - name: DEMO_GREETING\n  ##    value: \"Hello from the environment\"\n  ##\n  ## e.g. secret environment variable:\n  ## - name: USERNAME\n  ##   valueFrom:\n  ##     secretKeyRef:\n  ##       name: mysecret\n  ##       key: username\n  env: []\n\n  extraFlags:\n    - web.enable-lifecycle\n    ## web.enable-admin-api flag controls access to the administrative HTTP API which includes functionality such as\n    ## deleting time series. This is disabled by default.\n    # - web.enable-admin-api\n    ##\n    ## storage.tsdb.no-lockfile flag controls BD locking\n    # - storage.tsdb.no-lockfile\n    ##\n    ## storage.tsdb.wal-compression flag enables compression of the write-ahead log (WAL)\n    # - storage.tsdb.wal-compression\n\n  ## Path to a configuration file on prometheus server container FS\n  configPath: /etc/config/prometheus.yml\n\n  ### The data directory used by prometheus to set --storage.tsdb.path\n  ### When empty server.persistentVolume.mountPath is used instead\n  storagePath: \"\"\n\n  global:\n    ## How frequently to scrape targets by default\n    ##\n    scrape_interval: 1m\n    ## How long until a scrape request times out\n    ##\n    scrape_timeout: 10s\n    ## How frequently to evaluate rules\n    ##\n    evaluation_interval: 1m\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write\n  ##\n  remoteWrite: []\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_read\n  ##\n  remoteRead: []\n\n  ## Custom HTTP headers for Liveness/Readiness/Startup Probe\n  ##\n  ## Useful for providing HTTP Basic Auth to healthchecks\n  probeHeaders: []\n\n  ## Additional Prometheus server container arguments\n  ##\n  extraArgs: {}\n\n  ## Additional InitContainers to initialize the pod\n  ##\n  extraInitContainers: []\n\n  ## Additional Prometheus server Volume mounts\n  ##\n  extraVolumeMounts: []\n\n  ## Additional Prometheus server Volumes\n  ##\n  extraVolumes: []\n\n  ## Additional Prometheus server hostPath mounts\n  ##\n  extraHostPathMounts: []\n    # - name: certs-dir\n    #   mountPath: /etc/kubernetes/certs\n    #   subPath: \"\"\n    #   hostPath: /etc/kubernetes/certs\n    #   readOnly: true\n\n  extraConfigmapMounts: []\n    # - name: certs-configmap\n    #   mountPath: /prometheus\n    #   subPath: \"\"\n    #   configMap: certs-configmap\n    #   readOnly: true\n\n  ## Additional Prometheus server Secret mounts\n  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.\n  extraSecretMounts: []\n    # - name: secret-files\n    #   mountPath: /etc/secrets\n    #   subPath: \"\"\n    #   secretName: prom-secret-files\n    #   readOnly: true\n\n  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}}\n  ## Defining configMapOverrideName will cause templates/server-configmap.yaml\n  ## to NOT generate a ConfigMap resource\n  ##\n  configMapOverrideName: \"\"\n\n  ingress:\n    ## If true, Prometheus server Ingress will be created\n    ##\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    ## Prometheus server Ingress annotations\n    ##\n    annotations: {}\n    #   kubernetes.io/ingress.class: nginx\n    #   kubernetes.io/tls-acme: 'true'\n\n    ## Prometheus server Ingress additional labels\n    ##\n    extraLabels: {}\n\n    ## Prometheus server Ingress hostnames with optional path\n    ## Must be provided if Ingress is enabled\n    ##\n    hosts: []\n    #   - prometheus.domain.com\n    #   - domain.com/prometheus\n\n    path: /\n\n    # pathType is only for k8s \u003e= 1.18\n    pathType: Prefix\n\n    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.\n    extraPaths: []\n    # - path: /*\n    #   backend:\n    #     serviceName: ssl-redirect\n    #     servicePort: use-annotation\n\n    ## Prometheus server Ingress TLS configuration\n    ## Secrets must be manually created in the namespace\n    ##\n    tls: []\n    #   - secretName: prometheus-server-tls\n    #     hosts:\n    #       - prometheus.domain.com\n\n  ## Server Deployment Strategy type\n  # strategy:\n  #   type: Recreate\n\n  ## hostAliases allows adding entries to /etc/hosts inside the containers\n  hostAliases: []\n  #   - ip: \"127.0.0.1\"\n  #     hostnames:\n  #       - \"example.com\"\n\n  ## Node tolerations for server scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal|Exists\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  ## Node labels for Prometheus server pod assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Pod affinity\n  ##\n  affinity: {}\n\n  ## PodDisruptionBudget settings\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n  ##\n  podDisruptionBudget:\n    enabled: false\n    maxUnavailable: 1\n\n  ## Use an alternate scheduler, e.g. \"stork\".\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  # schedulerName:\n\n  persistentVolume:\n    ## If true, Prometheus server will create/use a Persistent Volume Claim\n    ## If false, use emptyDir\n    ##\n    enabled: true\n\n    ## Prometheus server data Persistent Volume access modes\n    ## Must match those of existing PV or dynamic provisioner\n    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    ##\n    accessModes:\n      - ReadWriteOnce\n\n    ## Prometheus server data Persistent Volume annotations\n    ##\n    annotations: {}\n\n    ## Prometheus server data Persistent Volume existing claim name\n    ## Requires server.persistentVolume.enabled: true\n    ## If defined, PVC must be created manually before volume will be bound\n    existingClaim: \"\"\n\n    ## Prometheus server data Persistent Volume mount root path\n    ##\n    mountPath: /data\n\n    ## Prometheus server data Persistent Volume size\n    ##\n    size: 8Gi\n\n    ## Prometheus server data Persistent Volume Storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    # storageClass: \"-\"\n\n    ## Prometheus server data Persistent Volume Binding Mode\n    ## If defined, volumeBindingMode: \u003cvolumeBindingMode\u003e\n    ## If undefined (the default) or set to null, no volumeBindingMode spec is\n    ##   set, choosing the default mode.\n    ##\n    # volumeBindingMode: \"\"\n\n    ## Subdirectory of Prometheus server data Persistent Volume to mount\n    ## Useful if the volume's root directory is not empty\n    ##\n    subPath: \"\"\n\n    ## Persistent Volume Claim Selector\n    ## Useful if Persistent Volumes have been provisioned in advance\n    ## Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector\n    ##\n    # selector:\n    #  matchLabels:\n    #    release: \"stable\"\n    #  matchExpressions:\n    #    - { key: environment, operator: In, values: [ dev ] }\n\n  emptyDir:\n    ## Prometheus server emptyDir volume size limit\n    ##\n    sizeLimit: \"\"\n\n  ## Annotations to be added to Prometheus server pods\n  ##\n  podAnnotations: {}\n    # iam.amazonaws.com/role: prometheus\n\n  ## Labels to be added to Prometheus server pods\n  ##\n  podLabels: {}\n\n  ## Prometheus AlertManager configuration\n  ##\n  alertmanagers: []\n\n  ## Specify if a Pod Security Policy for node-exporter must be created\n  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  ##\n  podSecurityPolicy:\n    annotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)\n  ##\n  replicaCount: 1\n\n  ## Annotations to be added to deployment\n  ##\n  deploymentAnnotations: {}\n\n  statefulSet:\n    ## If true, use a statefulset instead of a deployment for pod management.\n    ## This allows to scale replicas to more than 1 pod\n    ##\n    enabled: false\n\n    annotations: {}\n    labels: {}\n    podManagementPolicy: OrderedReady\n\n    ## Alertmanager headless service to use for the statefulset\n    ##\n    headless:\n      annotations: {}\n      labels: {}\n      servicePort: 80\n      ## Enable gRPC port on service to allow auto discovery with thanos-querier\n      gRPC:\n        enabled: false\n        servicePort: 10901\n        # nodePort: 10901\n\n  ## Prometheus server readiness and liveness probe initial delay and timeout\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n  ##\n  tcpSocketProbeEnabled: false\n  probeScheme: HTTP\n  readinessProbeInitialDelay: 30\n  readinessProbePeriodSeconds: 5\n  readinessProbeTimeout: 4\n  readinessProbeFailureThreshold: 3\n  readinessProbeSuccessThreshold: 1\n  livenessProbeInitialDelay: 30\n  livenessProbePeriodSeconds: 15\n  livenessProbeTimeout: 10\n  livenessProbeFailureThreshold: 3\n  livenessProbeSuccessThreshold: 1\n  startupProbe:\n    enabled: false\n    periodSeconds: 5\n    failureThreshold: 30\n    timeoutSeconds: 10\n\n  ## Prometheus server resource requests and limits\n  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n    # limits:\n    #   cpu: 500m\n    #   memory: 512Mi\n    # requests:\n    #   cpu: 500m\n    #   memory: 512Mi\n\n  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n  ##\n  hostNetwork: false\n\n  # When hostNetwork is enabled, you probably want to set this to ClusterFirstWithHostNet\n  dnsPolicy: ClusterFirst\n\n  ## Vertical Pod Autoscaler config\n  ## Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler\n  verticalAutoscaler:\n    ## If true a VPA object will be created for the controller (either StatefulSet or Deployemnt, based on above configs)\n    enabled: false\n    # updateMode: \"Auto\"\n    # containerPolicies:\n    # - containerName: 'prometheus-server'\n\n  # Custom DNS configuration to be added to prometheus server pods\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n  ## Security context to be added to server pods\n  ##\n  securityContext:\n    runAsUser: 65534\n    runAsNonRoot: true\n    runAsGroup: 65534\n    fsGroup: 65534\n\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 80\n    sessionAffinity: None\n    type: ClusterIP\n\n    ## Enable gRPC port on service to allow auto discovery with thanos-querier\n    gRPC:\n      enabled: false\n      servicePort: 10901\n      # nodePort: 10901\n\n    ## If using a statefulSet (statefulSet.enabled=true), configure the\n    ## service to connect to a specific replica to have a consistent view\n    ## of the data.\n    statefulsetReplica:\n      enabled: false\n      replica: 0\n\n  ## Prometheus server pod termination grace period\n  ##\n  terminationGracePeriodSeconds: 300\n\n  ## Prometheus data retention period (default if not specified is 15 days)\n  ##\n  retention: \"15d\"\n\npushgateway:\n  ## If false, pushgateway will not be installed\n  ##\n  enabled: true\n\n  ## Use an alternate scheduler, e.g. \"stork\".\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  # schedulerName:\n\n  ## pushgateway container name\n  ##\n  name: pushgateway\n\n  ## pushgateway container image\n  ##\n  image:\n    repository: prom/pushgateway\n    tag: v1.4.2\n    pullPolicy: IfNotPresent\n\n  ## pushgateway priorityClassName\n  ##\n  priorityClassName: \"\"\n\n  ## Additional pushgateway container arguments\n  ##\n  ## for example: persistence.file: /data/pushgateway.data\n  extraArgs: {}\n\n  ## Additional InitContainers to initialize the pod\n  ##\n  extraInitContainers: []\n\n  ingress:\n    ## If true, pushgateway Ingress will be created\n    ##\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    ## pushgateway Ingress annotations\n    ##\n    annotations: {}\n    #   kubernetes.io/ingress.class: nginx\n    #   kubernetes.io/tls-acme: 'true'\n\n    ## pushgateway Ingress hostnames with optional path\n    ## Must be provided if Ingress is enabled\n    ##\n    hosts: []\n    #   - pushgateway.domain.com\n    #   - domain.com/pushgateway\n\n    path: /\n\n    # pathType is only for k8s \u003e= 1.18\n    pathType: Prefix\n\n    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.\n    extraPaths: []\n    # - path: /*\n    #   backend:\n    #     serviceName: ssl-redirect\n    #     servicePort: use-annotation\n\n    ## pushgateway Ingress TLS configuration\n    ## Secrets must be manually created in the namespace\n    ##\n    tls: []\n    #   - secretName: prometheus-alerts-tls\n    #     hosts:\n    #       - pushgateway.domain.com\n\n  ## Node tolerations for pushgateway scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal|Exists\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  ## Node labels for pushgateway pod assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Annotations to be added to pushgateway pods\n  ##\n  podAnnotations: {}\n\n  ## Labels to be added to pushgateway pods\n  ##\n  podLabels: {}\n\n  ## Specify if a Pod Security Policy for node-exporter must be created\n  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  ##\n  podSecurityPolicy:\n    annotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  replicaCount: 1\n\n  ## Annotations to be added to deployment\n  ##\n  deploymentAnnotations: {}\n\n  ## PodDisruptionBudget settings\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n  ##\n  podDisruptionBudget:\n    enabled: false\n    maxUnavailable: 1\n\n  ## pushgateway resource requests and limits\n  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n    # limits:\n    #   cpu: 10m\n    #   memory: 32Mi\n    # requests:\n    #   cpu: 10m\n    #   memory: 32Mi\n\n  ## Vertical Pod Autoscaler config\n  ## Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler\n  verticalAutoscaler:\n    ## If true a VPA object will be created for the controller\n    enabled: false\n    # updateMode: \"Auto\"\n    # containerPolicies:\n    # - containerName: 'prometheus-pushgateway'\n\n  # Custom DNS configuration to be added to push-gateway pods\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n\n  ## Security context to be added to push-gateway pods\n  ##\n  securityContext:\n    runAsUser: 65534\n    runAsNonRoot: true\n\n  service:\n    annotations:\n      prometheus.io/probe: pushgateway\n    labels: {}\n    clusterIP: \"\"\n\n    ## List of IP addresses at which the pushgateway service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 9091\n    type: ClusterIP\n\n  ## pushgateway Deployment Strategy type\n  # strategy:\n  #   type: Recreate\n\n  persistentVolume:\n    ## If true, pushgateway will create/use a Persistent Volume Claim\n    ##\n    enabled: false\n\n    ## pushgateway data Persistent Volume access modes\n    ## Must match those of existing PV or dynamic provisioner\n    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    ##\n    accessModes:\n      - ReadWriteOnce\n\n    ## pushgateway data Persistent Volume Claim annotations\n    ##\n    annotations: {}\n\n    ## pushgateway data Persistent Volume existing claim name\n    ## Requires pushgateway.persistentVolume.enabled: true\n    ## If defined, PVC must be created manually before volume will be bound\n    existingClaim: \"\"\n\n    ## pushgateway data Persistent Volume mount root path\n    ##\n    mountPath: /data\n\n    ## pushgateway data Persistent Volume size\n    ##\n    size: 2Gi\n\n    ## pushgateway data Persistent Volume Storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    # storageClass: \"-\"\n\n    ## pushgateway data Persistent Volume Binding Mode\n    ## If defined, volumeBindingMode: \u003cvolumeBindingMode\u003e\n    ## If undefined (the default) or set to null, no volumeBindingMode spec is\n    ##   set, choosing the default mode.\n    ##\n    # volumeBindingMode: \"\"\n\n    ## Subdirectory of pushgateway data Persistent Volume to mount\n    ## Useful if the volume's root directory is not empty\n    ##\n    subPath: \"\"\n\n\n## alertmanager ConfigMap entries\n##\nalertmanagerFiles:\n  alertmanager.yml:\n    global: {}\n      # slack_api_url: ''\n\n    receivers:\n      - name: default-receiver\n        # slack_configs:\n        #  - channel: '@you'\n        #    send_resolved: true\n\n    route:\n      group_wait: 10s\n      group_interval: 5m\n      receiver: default-receiver\n      repeat_interval: 3h\n\n## Prometheus server ConfigMap entries\n##\nserverFiles:\n\n  ## Alerts configuration\n  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/\n  alerting_rules.yml:\n    groups:\n    - name: demo alert\n      rules:\n      - alert: High Pod Memory\n        expr: sum(container_memory_usage_bytes) \u003e 10\n        for: 1m\n        labels:\n          severity: slack\n        annotations:\n          summary: High Memory Usage\n  # groups:\n  #   - name: Instances\n  #     rules:\n  #       - alert: InstanceDown\n  #         expr: up == 0\n  #         for: 5m\n  #         labels:\n  #           severity: page\n  #         annotations:\n  #           description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'\n  #           summary: 'Instance {{ $labels.instance }} down'\n  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use alerting_rules.yml\n  alerts: {}\n\n  ## Records configuration\n  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/\n  recording_rules.yml: {}\n  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use recording_rules.yml\n  rules: {}\n\n  prometheus.yml:\n    rule_files:\n      - /etc/config/recording_rules.yml\n      - /etc/config/alerting_rules.yml\n    ## Below two files are DEPRECATED will be removed from this default values file\n      - /etc/config/rules\n      - /etc/config/alerts\n\n    scrape_configs:\n      - job_name: prometheus\n        static_configs:\n          - targets:\n            - localhost:9090\n\n      # A scrape configuration for running Prometheus on a Kubernetes cluster.\n      # This uses separate scrape configs for cluster components (i.e. API server, node)\n      # and services to allow each to use different authentication configs.\n      #\n      # Kubernetes labels will be added as Prometheus labels on metrics via the\n      # `labelmap` relabeling action.\n\n      # Scrape config for API servers.\n      #\n      # Kubernetes exposes API servers as endpoints to the default/kubernetes\n      # service so this uses `endpoints` role and uses relabelling to only keep\n      # the endpoints associated with the default/kubernetes service using the\n      # default named port `https`. This works for single API server deployments as\n      # well as HA API server deployments.\n      - job_name: 'kubernetes-apiservers'\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        # Keep only the default/kubernetes service endpoints for the https port. This\n        # will add targets for each API server which Kubernetes adds an endpoint to\n        # the default/kubernetes service.\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n            action: keep\n            regex: default;kubernetes;https\n\n      - job_name: 'kubernetes-nodes'\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        kubernetes_sd_configs:\n          - role: node\n\n        relabel_configs:\n          - action: labelmap\n            regex: __meta_kubernetes_node_label_(.+)\n          - target_label: __address__\n            replacement: kubernetes.default.svc:443\n          - source_labels: [__meta_kubernetes_node_name]\n            regex: (.+)\n            target_label: __metrics_path__\n            replacement: /api/v1/nodes/$1/proxy/metrics\n\n\n      - job_name: 'kubernetes-nodes-cadvisor'\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        kubernetes_sd_configs:\n          - role: node\n\n        # This configuration will work only on kubelet 1.7.3+\n        # As the scrape endpoints for cAdvisor have changed\n        # if you are using older version you need to change the replacement to\n        # replacement: /api/v1/nodes/$1:4194/proxy/metrics\n        # more info here https://github.com/coreos/prometheus-operator/issues/633\n        relabel_configs:\n          - action: labelmap\n            regex: __meta_kubernetes_node_label_(.+)\n          - target_label: __address__\n            replacement: kubernetes.default.svc:443\n          - source_labels: [__meta_kubernetes_node_name]\n            regex: (.+)\n            target_label: __metrics_path__\n            replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor\n\n      # Scrape config for service endpoints.\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/scrape`: Only scrape services that have a value of\n      # `true`, except if `prometheus.io/scrape-slow` is set to `true` as well.\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: If the metrics are exposed on a different port to the\n      # service then set this appropriately.\n      # * `prometheus.io/param_\u003cparameter\u003e`: If the metrics endpoint uses parameters\n      # then you can set any parameter\n      - job_name: 'kubernetes-service-endpoints'\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]\n            action: drop\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n            action: replace\n            target_label: __scheme__\n            regex: (https?)\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n            action: replace\n            target_label: __address__\n            regex: ([^:]+)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n          - action: labelmap\n            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            action: replace\n            target_label: service\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n      # Scrape config for slow service endpoints; same as above, but with a larger\n      # timeout and a larger interval\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: If the metrics are exposed on a different port to the\n      # service then set this appropriately.\n      # * `prometheus.io/param_\u003cparameter\u003e`: If the metrics endpoint uses parameters\n      # then you can set any parameter\n      - job_name: 'kubernetes-service-endpoints-slow'\n\n        scrape_interval: 5m\n        scrape_timeout: 30s\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n            action: replace\n            target_label: __scheme__\n            regex: (https?)\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n            action: replace\n            target_label: __address__\n            regex: ([^:]+)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n          - action: labelmap\n            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            action: replace\n            target_label: service\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n      - job_name: 'prometheus-pushgateway'\n        honor_labels: true\n\n        kubernetes_sd_configs:\n          - role: service\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n            action: keep\n            regex: pushgateway\n\n      # Example scrape config for probing services via the Blackbox Exporter.\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/probe`: Only probe services that have a value of `true`\n      - job_name: 'kubernetes-services'\n\n        metrics_path: /probe\n        params:\n          module: [http_2xx]\n\n        kubernetes_sd_configs:\n          - role: service\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n            action: keep\n            regex: true\n          - source_labels: [__address__]\n            target_label: __param_target\n          - target_label: __address__\n            replacement: blackbox\n          - source_labels: [__param_target]\n            target_label: instance\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            target_label: service\n\n      # Example scrape config for pods\n      #\n      # The relabeling allows the actual pod scrape endpoint to be configured via the\n      # following annotations:\n      #\n      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`,\n      # except if `prometheus.io/scrape-slow` is set to `true` as well.\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.\n      - job_name: 'kubernetes-pods'\n\n        kubernetes_sd_configs:\n          - role: pod\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]\n            action: drop\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]\n            action: replace\n            regex: (https?)\n            target_label: __scheme__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n            action: replace\n            regex: ([^:]+)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n            target_label: __address__\n          - action: labelmap\n            regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_pod_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_pod_name]\n            action: replace\n            target_label: pod\n          - source_labels: [__meta_kubernetes_pod_phase]\n            regex: Pending|Succeeded|Failed|Completed\n            action: drop\n\n      # Example Scrape config for pods which should be scraped slower. An useful example\n      # would be stackriver-exporter which queries an API on every scrape of the pod\n      #\n      # The relabeling allows the actual pod scrape endpoint to be configured via the\n      # following annotations:\n      #\n      # * `prometheus.io/scrape-slow`: Only scrape pods that have a value of `true`\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.\n      - job_name: 'kubernetes-pods-slow'\n\n        scrape_interval: 5m\n        scrape_timeout: 30s\n\n        kubernetes_sd_configs:\n          - role: pod\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]\n            action: replace\n            regex: (https?)\n            target_label: __scheme__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n            action: replace\n            regex: ([^:]+)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n            target_label: __address__\n          - action: labelmap\n            regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_pod_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_pod_name]\n            action: replace\n            target_label: pod\n          - source_labels: [__meta_kubernetes_pod_phase]\n            regex: Pending|Succeeded|Failed|Completed\n            action: drop\n\n# adds additional scrape configs to prometheus.yml\n# must be a string so you have to add a | after extraScrapeConfigs:\n# example adds prometheus-blackbox-exporter scrape config\nextraScrapeConfigs:\n  # - job_name: 'prometheus-blackbox-exporter'\n  #   metrics_path: /probe\n  #   params:\n  #     module: [http_2xx]\n  #   static_configs:\n  #     - targets:\n  #       - https://example.com\n  #   relabel_configs:\n  #     - source_labels: [__address__]\n  #       target_label: __param_target\n  #     - source_labels: [__param_target]\n  #       target_label: instance\n  #     - target_label: __address__\n  #       replacement: prometheus-blackbox-exporter:9115\n\n# Adds option to add alert_relabel_configs to avoid duplicate alerts in alertmanager\n# useful in H/A prometheus with different external labels but the same alerts\nalertRelabelConfigs:\n  # alert_relabel_configs:\n  # - source_labels: [dc]\n  #   regex: (.+)\\d+\n  #   target_label: dc\n\nnetworkPolicy:\n  ## Enable creation of NetworkPolicy resources.\n  ##\n  enabled: false\n\n# Force namespace of namespaced resources\nforceNamespace: null\n",
            "template": "rbac:\n  create: true\n\npodSecurityPolicy:\n  enabled: false\n\nimagePullSecrets:\n# - name: \"image-pull-secret\"\n\n## Define serviceAccount names for components. Defaults to component's fully qualified name.\n##\nserviceAccounts:\n  alertmanager:\n    create: true\n    name:\n    annotations: {}\n  nodeExporter:\n    create: true\n    name:\n    annotations: {}\n  pushgateway:\n    create: true\n    name:\n    annotations: {}\n  server:\n    create: true\n    name:\n    annotations: {}\n\nalertmanager:\n  ## If false, alertmanager will not be installed\n  ##\n  enabled: true\n\n  ## Use a ClusterRole (and ClusterRoleBinding)\n  ## - If set to false - we define a Role and RoleBinding in the defined namespaces ONLY\n  ## This makes alertmanager work - for users who do not have ClusterAdmin privs, but wants alertmanager to operate on their own namespaces, instead of clusterwide.\n  useClusterRole: true\n\n  ## Set to a rolename to use existing role - skipping role creating - but still doing serviceaccount and rolebinding to the rolename set here.\n  useExistingRole: false\n\n  ## alertmanager container name\n  ##\n  name: alertmanager\n\n  ## alertmanager container image\n  ##\n  image:\n    repository: quay.io/prometheus/alertmanager\n    tag: v0.23.0\n    pullPolicy: IfNotPresent\n\n  ## alertmanager priorityClassName\n  ##\n  priorityClassName: \"\"\n\n  ## Additional alertmanager container arguments\n  ##\n  extraArgs: {}\n\n  ## Additional InitContainers to initialize the pod\n  ##\n  extraInitContainers: []\n\n  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug\n  ## so that the various internal URLs are still able to access as they are in the default case.\n  ## (Optional)\n  prefixURL: \"\"\n\n  ## External URL which can access alertmanager\n  baseURL: \"http://localhost:9093\"\n\n  ## Additional alertmanager container environment variable\n  ## For instance to add a http_proxy\n  ##\n  extraEnv: {}\n\n  ## Additional alertmanager Secret mounts\n  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.\n  extraSecretMounts: []\n    # - name: secret-files\n    #   mountPath: /etc/secrets\n    #   subPath: \"\"\n    #   secretName: alertmanager-secret-files\n    #   readOnly: true\n\n  ## Additional alertmanager Configmap mounts\n  extraConfigmapMounts: []\n    # - name: template-files\n    #   mountPath: /etc/config/templates.d\n    #   configMap: alertmanager-template-files\n    #   readOnly: true\n\n  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.alertmanager.configMapOverrideName}}\n  ## Defining configMapOverrideName will cause templates/alertmanager-configmap.yaml\n  ## to NOT generate a ConfigMap resource\n  ##\n  configMapOverrideName: \"\"\n\n  ## The name of a secret in the same kubernetes namespace which contains the Alertmanager config\n  ## Defining configFromSecret will cause templates/alertmanager-configmap.yaml\n  ## to NOT generate a ConfigMap resource\n  ##\n  configFromSecret: \"\"\n\n  ## The configuration file name to be loaded to alertmanager\n  ## Must match the key within configuration loaded from ConfigMap/Secret\n  ##\n  configFileName: alertmanager.yml\n\n  ingress:\n    ## If true, alertmanager Ingress will be created\n    ##\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    ## alertmanager Ingress annotations\n    ##\n    annotations: {}\n    #   kubernetes.io/ingress.class: nginx\n    #   kubernetes.io/tls-acme: 'true'\n\n    ## alertmanager Ingress additional labels\n    ##\n    extraLabels: {}\n\n    ## alertmanager Ingress hostnames with optional path\n    ## Must be provided if Ingress is enabled\n    ##\n    hosts: []\n    #   - alertmanager.domain.com\n    #   - domain.com/alertmanager\n\n    path: /\n\n    # pathType is only for k8s \u003e= 1.18\n    pathType: Prefix\n\n    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.\n    extraPaths: []\n    # - path: /*\n    #   backend:\n    #     serviceName: ssl-redirect\n    #     servicePort: use-annotation\n\n    ## alertmanager Ingress TLS configuration\n    ## Secrets must be manually created in the namespace\n    ##\n    tls: []\n    #   - secretName: prometheus-alerts-tls\n    #     hosts:\n    #       - alertmanager.domain.com\n\n  ## Alertmanager Deployment Strategy type\n  # strategy:\n  #   type: Recreate\n\n  ## Node tolerations for alertmanager scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal|Exists\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  ## Node labels for alertmanager pod assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Pod affinity\n  ##\n  affinity: {}\n\n  ## PodDisruptionBudget settings\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n  ##\n  podDisruptionBudget:\n    enabled: false\n    maxUnavailable: 1\n\n  ## Use an alternate scheduler, e.g. \"stork\".\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  # schedulerName:\n\n  persistentVolume:\n    ## If true, alertmanager will create/use a Persistent Volume Claim\n    ## If false, use emptyDir\n    ##\n    enabled: true\n\n    ## alertmanager data Persistent Volume access modes\n    ## Must match those of existing PV or dynamic provisioner\n    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    ##\n    accessModes:\n      - ReadWriteOnce\n\n    ## alertmanager data Persistent Volume Claim annotations\n    ##\n    annotations: {}\n\n    ## alertmanager data Persistent Volume existing claim name\n    ## Requires alertmanager.persistentVolume.enabled: true\n    ## If defined, PVC must be created manually before volume will be bound\n    existingClaim: \"\"\n\n    ## alertmanager data Persistent Volume mount root path\n    ##\n    mountPath: /data\n\n    ## alertmanager data Persistent Volume size\n    ##\n    size: 2Gi\n\n    ## alertmanager data Persistent Volume Storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    # storageClass: \"-\"\n\n    ## alertmanager data Persistent Volume Binding Mode\n    ## If defined, volumeBindingMode: \u003cvolumeBindingMode\u003e\n    ## If undefined (the default) or set to null, no volumeBindingMode spec is\n    ##   set, choosing the default mode.\n    ##\n    # volumeBindingMode: \"\"\n\n    ## Subdirectory of alertmanager data Persistent Volume to mount\n    ## Useful if the volume's root directory is not empty\n    ##\n    subPath: \"\"\n\n    ## Persistent Volume Claim Selector\n    ## Useful if Persistent Volumes have been provisioned in advance\n    ## Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector\n    ##\n    # selector:\n    #  matchLabels:\n    #    release: \"stable\"\n    #  matchExpressions:\n    #    - { key: environment, operator: In, values: [ dev ] }\n\n  emptyDir:\n    ## alertmanager emptyDir volume size limit\n    ##\n    sizeLimit: \"\"\n\n  ## Annotations to be added to alertmanager pods\n  ##\n  podAnnotations: {}\n    ## Tell prometheus to use a specific set of alertmanager pods\n    ## instead of all alertmanager pods found in the same namespace\n    ## Useful if you deploy multiple releases within the same namespace\n    ##\n    ## prometheus.io/probe: alertmanager-teamA\n\n  ## Labels to be added to Prometheus AlertManager pods\n  ##\n  podLabels: {}\n\n  ## Specify if a Pod Security Policy for node-exporter must be created\n  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  ##\n  podSecurityPolicy:\n    annotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)\n  ##\n  replicaCount: 1\n\n  ## Annotations to be added to deployment\n  ##\n  deploymentAnnotations: {}\n\n  statefulSet:\n    ## If true, use a statefulset instead of a deployment for pod management.\n    ## This allows to scale replicas to more than 1 pod\n    ##\n    enabled: false\n\n    annotations: {}\n    labels: {}\n    podManagementPolicy: OrderedReady\n\n    ## Alertmanager headless service to use for the statefulset\n    ##\n    headless:\n      annotations: {}\n      labels: {}\n\n      ## Enabling peer mesh service end points for enabling the HA alert manager\n      ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md\n      enableMeshPeer: false\n\n      servicePort: 80\n\n  ## alertmanager resource requests and limits\n  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n    # limits:\n    #   cpu: 10m\n    #   memory: 32Mi\n    # requests:\n    #   cpu: 10m\n    #   memory: 32Mi\n\n  # Custom DNS configuration to be added to alertmanager pods\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n\n  ## Security context to be added to alertmanager pods\n  ##\n  securityContext:\n    runAsUser: 65534\n    runAsNonRoot: true\n    runAsGroup: 65534\n    fsGroup: 65534\n\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n    ## Enabling peer mesh service end points for enabling the HA alert manager\n    ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md\n    # enableMeshPeer : true\n\n    ## List of IP addresses at which the alertmanager service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 80\n    # nodePort: 30000\n    sessionAffinity: None\n    type: ClusterIP\n\n  ## List of initial peers\n  ## Ref: https://github.com/prometheus/alertmanager/blob/main/README.md#high-availability\n  clusterPeers: []\n\n## Monitors ConfigMap changes and POSTs to a URL\n## Ref: https://github.com/jimmidyson/configmap-reload\n##\nconfigmapReload:\n  prometheus:\n    ## If false, the configmap-reload container will not be deployed\n    ##\n    enabled: true\n\n    ## configmap-reload container name\n    ##\n    name: configmap-reload\n\n    ## configmap-reload container image\n    ##\n    image:\n      repository: jimmidyson/configmap-reload\n      tag: v0.5.0\n      pullPolicy: IfNotPresent\n\n    ## Additional configmap-reload container arguments\n    ##\n    extraArgs: {}\n    ## Additional configmap-reload volume directories\n    ##\n    extraVolumeDirs: []\n\n\n    ## Additional configmap-reload mounts\n    ##\n    extraConfigmapMounts: []\n      # - name: prometheus-alerts\n      #   mountPath: /etc/alerts.d\n      #   subPath: \"\"\n      #   configMap: prometheus-alerts\n      #   readOnly: true\n\n\n    ## configmap-reload resource requests and limits\n    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n  alertmanager:\n    ## If false, the configmap-reload container will not be deployed\n    ##\n    enabled: true\n\n    ## configmap-reload container name\n    ##\n    name: configmap-reload\n\n    ## configmap-reload container image\n    ##\n    image:\n      repository: jimmidyson/configmap-reload\n      tag: v0.5.0\n      pullPolicy: IfNotPresent\n\n    ## Additional configmap-reload container arguments\n    ##\n    extraArgs: {}\n    ## Additional configmap-reload volume directories\n    ##\n    extraVolumeDirs: []\n\n\n    ## Additional configmap-reload mounts\n    ##\n    extraConfigmapMounts: []\n      # - name: prometheus-alerts\n      #   mountPath: /etc/alerts.d\n      #   subPath: \"\"\n      #   configMap: prometheus-alerts\n      #   readOnly: true\n\n\n    ## configmap-reload resource requests and limits\n    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n\nkubeStateMetrics:\n  ## If false, kube-state-metrics sub-chart will not be installed\n  ##\n  enabled: true\n\n## kube-state-metrics sub-chart configurable values\n## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics\n##\n# kube-state-metrics:\n\nnodeExporter:\n  ## If false, node-exporter will not be installed\n  ##\n  enabled: true\n\n  ## If true, node-exporter pods share the host network namespace\n  ##\n  hostNetwork: true\n\n  ## If true, node-exporter pods share the host PID namespace\n  ##\n  hostPID: true\n\n  ## If true, node-exporter pods mounts host / at /host/root\n  ##\n  hostRootfs: true\n\n  ## node-exporter container name\n  ##\n  name: node-exporter\n\n  ## node-exporter container image\n  ##\n  image:\n    repository: quay.io/prometheus/node-exporter\n    tag: v1.3.0\n    pullPolicy: IfNotPresent\n\n  ## Specify if a Pod Security Policy for node-exporter must be created\n  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  ##\n  podSecurityPolicy:\n    annotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## node-exporter priorityClassName\n  ##\n  priorityClassName: \"\"\n\n  ## Custom Update Strategy\n  ##\n  updateStrategy:\n    type: RollingUpdate\n\n  ## Additional node-exporter container arguments\n  ##\n  extraArgs: {}\n\n  ## Additional InitContainers to initialize the pod\n  ##\n  extraInitContainers: []\n\n  ## Additional node-exporter hostPath mounts\n  ##\n  extraHostPathMounts: []\n    # - name: textfile-dir\n    #   mountPath: /srv/txt_collector\n    #   hostPath: /var/lib/node-exporter\n    #   readOnly: true\n    #   mountPropagation: HostToContainer\n\n  extraConfigmapMounts: []\n    # - name: certs-configmap\n    #   mountPath: /prometheus\n    #   configMap: certs-configmap\n    #   readOnly: true\n\n  ## Node tolerations for node-exporter scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal|Exists\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  ## Node labels for node-exporter pod assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Annotations to be added to node-exporter pods\n  ##\n  podAnnotations: {}\n\n  ## Labels to be added to node-exporter pods\n  ##\n  pod:\n    labels: {}\n\n  ## PodDisruptionBudget settings\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n  ##\n  podDisruptionBudget:\n    enabled: false\n    maxUnavailable: 1\n\n  ## node-exporter resource limits \u0026 requests\n  ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n    # limits:\n    #   cpu: 200m\n    #   memory: 50Mi\n    # requests:\n    #   cpu: 100m\n    #   memory: 30Mi\n\n  # Custom DNS configuration to be added to node-exporter pods\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n\n  ## Security context to be added to node-exporter pods\n  ##\n  securityContext:\n    fsGroup: 65534\n    runAsGroup: 65534\n    runAsNonRoot: true\n    runAsUser: 65534\n\n  service:\n    annotations:\n      prometheus.io/scrape: \"true\"\n    labels: {}\n\n    # Exposed as a headless service:\n    # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services\n    clusterIP: None\n\n    ## List of IP addresses at which the node-exporter service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    hostPort: 9100\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 9100\n    type: ClusterIP\n\nserver:\n  ## Prometheus server container name\n  ##\n  enabled: true\n\n  ## Use a ClusterRole (and ClusterRoleBinding)\n  ## - If set to false - we define a RoleBinding in the defined namespaces ONLY\n  ##\n  ## NB: because we need a Role with nonResourceURL's (\"/metrics\") - you must get someone with Cluster-admin privileges to define this role for you, before running with this setting enabled.\n  ##     This makes prometheus work - for users who do not have ClusterAdmin privs, but wants prometheus to operate on their own namespaces, instead of clusterwide.\n  ##\n  ## You MUST also set namespaces to the ones you have access to and want monitored by Prometheus.\n  ##\n  # useExistingClusterRoleName: nameofclusterrole\n\n  ## namespaces to monitor (instead of monitoring all - clusterwide). Needed if you want to run without Cluster-admin privileges.\n  # namespaces:\n  #   - yournamespace\n\n  name: server\n\n  # sidecarContainers - add more containers to prometheus server\n  # Key/Value where Key is the sidecar `- name: \u003cKey\u003e`\n  # Example:\n  #   sidecarContainers:\n  #      webserver:\n  #        image: nginx\n  sidecarContainers: {}\n\n  # sidecarTemplateValues - context to be used in template for sidecarContainers\n  # Example:\n  #   sidecarTemplateValues: *your-custom-globals\n  #   sidecarContainers:\n  #     webserver: |-\n  #       {{ include \"webserver-container-template\" . }}\n  # Template for `webserver-container-template` might looks like this:\n  #   image: \"{{ .Values.server.sidecarTemplateValues.repository }}:{{ .Values.server.sidecarTemplateValues.tag }}\"\n  #   ...\n  #\n  sidecarTemplateValues: {}\n\n  ## Prometheus server container image\n  ##\n  image:\n    repository: quay.io/prometheus/prometheus\n    tag: v2.31.1\n    pullPolicy: IfNotPresent\n\n  ## prometheus server priorityClassName\n  ##\n  priorityClassName: \"\"\n\n  ## EnableServiceLinks indicates whether information about services should be injected\n  ## into pod's environment variables, matching the syntax of Docker links.\n  ## WARNING: the field is unsupported and will be skipped in K8s prior to v1.13.0.\n  ##\n  enableServiceLinks: true\n\n  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug\n  ## so that the various internal URLs are still able to access as they are in the default case.\n  ## (Optional)\n  prefixURL: \"\"\n\n  ## External URL which can access prometheus\n  ## Maybe same with Ingress host name\n  baseURL: \"\"\n\n  ## Additional server container environment variables\n  ##\n  ## You specify this manually like you would a raw deployment manifest.\n  ## This means you can bind in environment variables from secrets.\n  ##\n  ## e.g. static environment variable:\n  ##  - name: DEMO_GREETING\n  ##    value: \"Hello from the environment\"\n  ##\n  ## e.g. secret environment variable:\n  ## - name: USERNAME\n  ##   valueFrom:\n  ##     secretKeyRef:\n  ##       name: mysecret\n  ##       key: username\n  env: []\n\n  extraFlags:\n    - web.enable-lifecycle\n    ## web.enable-admin-api flag controls access to the administrative HTTP API which includes functionality such as\n    ## deleting time series. This is disabled by default.\n    # - web.enable-admin-api\n    ##\n    ## storage.tsdb.no-lockfile flag controls BD locking\n    # - storage.tsdb.no-lockfile\n    ##\n    ## storage.tsdb.wal-compression flag enables compression of the write-ahead log (WAL)\n    # - storage.tsdb.wal-compression\n\n  ## Path to a configuration file on prometheus server container FS\n  configPath: /etc/config/prometheus.yml\n\n  ### The data directory used by prometheus to set --storage.tsdb.path\n  ### When empty server.persistentVolume.mountPath is used instead\n  storagePath: \"\"\n\n  global:\n    ## How frequently to scrape targets by default\n    ##\n    scrape_interval: 1m\n    ## How long until a scrape request times out\n    ##\n    scrape_timeout: 10s\n    ## How frequently to evaluate rules\n    ##\n    evaluation_interval: 1m\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write\n  ##\n  remoteWrite: []\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_read\n  ##\n  remoteRead: []\n\n  ## Custom HTTP headers for Liveness/Readiness/Startup Probe\n  ##\n  ## Useful for providing HTTP Basic Auth to healthchecks\n  probeHeaders: []\n\n  ## Additional Prometheus server container arguments\n  ##\n  extraArgs: {}\n\n  ## Additional InitContainers to initialize the pod\n  ##\n  extraInitContainers: []\n\n  ## Additional Prometheus server Volume mounts\n  ##\n  extraVolumeMounts: []\n\n  ## Additional Prometheus server Volumes\n  ##\n  extraVolumes: []\n\n  ## Additional Prometheus server hostPath mounts\n  ##\n  extraHostPathMounts: []\n    # - name: certs-dir\n    #   mountPath: /etc/kubernetes/certs\n    #   subPath: \"\"\n    #   hostPath: /etc/kubernetes/certs\n    #   readOnly: true\n\n  extraConfigmapMounts: []\n    # - name: certs-configmap\n    #   mountPath: /prometheus\n    #   subPath: \"\"\n    #   configMap: certs-configmap\n    #   readOnly: true\n\n  ## Additional Prometheus server Secret mounts\n  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.\n  extraSecretMounts: []\n    # - name: secret-files\n    #   mountPath: /etc/secrets\n    #   subPath: \"\"\n    #   secretName: prom-secret-files\n    #   readOnly: true\n\n  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}}\n  ## Defining configMapOverrideName will cause templates/server-configmap.yaml\n  ## to NOT generate a ConfigMap resource\n  ##\n  configMapOverrideName: \"\"\n\n  ingress:\n    ## If true, Prometheus server Ingress will be created\n    ##\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    ## Prometheus server Ingress annotations\n    ##\n    annotations: {}\n    #   kubernetes.io/ingress.class: nginx\n    #   kubernetes.io/tls-acme: 'true'\n\n    ## Prometheus server Ingress additional labels\n    ##\n    extraLabels: {}\n\n    ## Prometheus server Ingress hostnames with optional path\n    ## Must be provided if Ingress is enabled\n    ##\n    hosts: []\n    #   - prometheus.domain.com\n    #   - domain.com/prometheus\n\n    path: /\n\n    # pathType is only for k8s \u003e= 1.18\n    pathType: Prefix\n\n    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.\n    extraPaths: []\n    # - path: /*\n    #   backend:\n    #     serviceName: ssl-redirect\n    #     servicePort: use-annotation\n\n    ## Prometheus server Ingress TLS configuration\n    ## Secrets must be manually created in the namespace\n    ##\n    tls: []\n    #   - secretName: prometheus-server-tls\n    #     hosts:\n    #       - prometheus.domain.com\n\n  ## Server Deployment Strategy type\n  # strategy:\n  #   type: Recreate\n\n  ## hostAliases allows adding entries to /etc/hosts inside the containers\n  hostAliases: []\n  #   - ip: \"127.0.0.1\"\n  #     hostnames:\n  #       - \"example.com\"\n\n  ## Node tolerations for server scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal|Exists\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  ## Node labels for Prometheus server pod assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Pod affinity\n  ##\n  affinity: {}\n\n  ## PodDisruptionBudget settings\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n  ##\n  podDisruptionBudget:\n    enabled: false\n    maxUnavailable: 1\n\n  ## Use an alternate scheduler, e.g. \"stork\".\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  # schedulerName:\n\n  persistentVolume:\n    ## If true, Prometheus server will create/use a Persistent Volume Claim\n    ## If false, use emptyDir\n    ##\n    enabled: true\n\n    ## Prometheus server data Persistent Volume access modes\n    ## Must match those of existing PV or dynamic provisioner\n    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    ##\n    accessModes:\n      - ReadWriteOnce\n\n    ## Prometheus server data Persistent Volume annotations\n    ##\n    annotations: {}\n\n    ## Prometheus server data Persistent Volume existing claim name\n    ## Requires server.persistentVolume.enabled: true\n    ## If defined, PVC must be created manually before volume will be bound\n    existingClaim: \"\"\n\n    ## Prometheus server data Persistent Volume mount root path\n    ##\n    mountPath: /data\n\n    ## Prometheus server data Persistent Volume size\n    ##\n    size: 8Gi\n\n    ## Prometheus server data Persistent Volume Storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    # storageClass: \"-\"\n\n    ## Prometheus server data Persistent Volume Binding Mode\n    ## If defined, volumeBindingMode: \u003cvolumeBindingMode\u003e\n    ## If undefined (the default) or set to null, no volumeBindingMode spec is\n    ##   set, choosing the default mode.\n    ##\n    # volumeBindingMode: \"\"\n\n    ## Subdirectory of Prometheus server data Persistent Volume to mount\n    ## Useful if the volume's root directory is not empty\n    ##\n    subPath: \"\"\n\n    ## Persistent Volume Claim Selector\n    ## Useful if Persistent Volumes have been provisioned in advance\n    ## Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector\n    ##\n    # selector:\n    #  matchLabels:\n    #    release: \"stable\"\n    #  matchExpressions:\n    #    - { key: environment, operator: In, values: [ dev ] }\n\n  emptyDir:\n    ## Prometheus server emptyDir volume size limit\n    ##\n    sizeLimit: \"\"\n\n  ## Annotations to be added to Prometheus server pods\n  ##\n  podAnnotations: {}\n    # iam.amazonaws.com/role: prometheus\n\n  ## Labels to be added to Prometheus server pods\n  ##\n  podLabels: {}\n\n  ## Prometheus AlertManager configuration\n  ##\n  alertmanagers: []\n\n  ## Specify if a Pod Security Policy for node-exporter must be created\n  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  ##\n  podSecurityPolicy:\n    annotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)\n  ##\n  replicaCount: 1\n\n  ## Annotations to be added to deployment\n  ##\n  deploymentAnnotations: {}\n\n  statefulSet:\n    ## If true, use a statefulset instead of a deployment for pod management.\n    ## This allows to scale replicas to more than 1 pod\n    ##\n    enabled: false\n\n    annotations: {}\n    labels: {}\n    podManagementPolicy: OrderedReady\n\n    ## Alertmanager headless service to use for the statefulset\n    ##\n    headless:\n      annotations: {}\n      labels: {}\n      servicePort: 80\n      ## Enable gRPC port on service to allow auto discovery with thanos-querier\n      gRPC:\n        enabled: false\n        servicePort: 10901\n        # nodePort: 10901\n\n  ## Prometheus server readiness and liveness probe initial delay and timeout\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n  ##\n  tcpSocketProbeEnabled: false\n  probeScheme: HTTP\n  readinessProbeInitialDelay: 30\n  readinessProbePeriodSeconds: 5\n  readinessProbeTimeout: 4\n  readinessProbeFailureThreshold: 3\n  readinessProbeSuccessThreshold: 1\n  livenessProbeInitialDelay: 30\n  livenessProbePeriodSeconds: 15\n  livenessProbeTimeout: 10\n  livenessProbeFailureThreshold: 3\n  livenessProbeSuccessThreshold: 1\n  startupProbe:\n    enabled: false\n    periodSeconds: 5\n    failureThreshold: 30\n    timeoutSeconds: 10\n\n  ## Prometheus server resource requests and limits\n  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n    # limits:\n    #   cpu: 500m\n    #   memory: 512Mi\n    # requests:\n    #   cpu: 500m\n    #   memory: 512Mi\n\n  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n  ##\n  hostNetwork: false\n\n  # When hostNetwork is enabled, you probably want to set this to ClusterFirstWithHostNet\n  dnsPolicy: ClusterFirst\n\n  ## Vertical Pod Autoscaler config\n  ## Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler\n  verticalAutoscaler:\n    ## If true a VPA object will be created for the controller (either StatefulSet or Deployemnt, based on above configs)\n    enabled: false\n    # updateMode: \"Auto\"\n    # containerPolicies:\n    # - containerName: 'prometheus-server'\n\n  # Custom DNS configuration to be added to prometheus server pods\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n  ## Security context to be added to server pods\n  ##\n  securityContext:\n    runAsUser: 65534\n    runAsNonRoot: true\n    runAsGroup: 65534\n    fsGroup: 65534\n\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 80\n    sessionAffinity: None\n    type: ClusterIP\n\n    ## Enable gRPC port on service to allow auto discovery with thanos-querier\n    gRPC:\n      enabled: false\n      servicePort: 10901\n      # nodePort: 10901\n\n    ## If using a statefulSet (statefulSet.enabled=true), configure the\n    ## service to connect to a specific replica to have a consistent view\n    ## of the data.\n    statefulsetReplica:\n      enabled: false\n      replica: 0\n\n  ## Prometheus server pod termination grace period\n  ##\n  terminationGracePeriodSeconds: 300\n\n  ## Prometheus data retention period (default if not specified is 15 days)\n  ##\n  retention: \"15d\"\n\npushgateway:\n  ## If false, pushgateway will not be installed\n  ##\n  enabled: true\n\n  ## Use an alternate scheduler, e.g. \"stork\".\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  # schedulerName:\n\n  ## pushgateway container name\n  ##\n  name: pushgateway\n\n  ## pushgateway container image\n  ##\n  image:\n    repository: prom/pushgateway\n    tag: v1.4.2\n    pullPolicy: IfNotPresent\n\n  ## pushgateway priorityClassName\n  ##\n  priorityClassName: \"\"\n\n  ## Additional pushgateway container arguments\n  ##\n  ## for example: persistence.file: /data/pushgateway.data\n  extraArgs: {}\n\n  ## Additional InitContainers to initialize the pod\n  ##\n  extraInitContainers: []\n\n  ingress:\n    ## If true, pushgateway Ingress will be created\n    ##\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    ## pushgateway Ingress annotations\n    ##\n    annotations: {}\n    #   kubernetes.io/ingress.class: nginx\n    #   kubernetes.io/tls-acme: 'true'\n\n    ## pushgateway Ingress hostnames with optional path\n    ## Must be provided if Ingress is enabled\n    ##\n    hosts: []\n    #   - pushgateway.domain.com\n    #   - domain.com/pushgateway\n\n    path: /\n\n    # pathType is only for k8s \u003e= 1.18\n    pathType: Prefix\n\n    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.\n    extraPaths: []\n    # - path: /*\n    #   backend:\n    #     serviceName: ssl-redirect\n    #     servicePort: use-annotation\n\n    ## pushgateway Ingress TLS configuration\n    ## Secrets must be manually created in the namespace\n    ##\n    tls: []\n    #   - secretName: prometheus-alerts-tls\n    #     hosts:\n    #       - pushgateway.domain.com\n\n  ## Node tolerations for pushgateway scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal|Exists\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  ## Node labels for pushgateway pod assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Annotations to be added to pushgateway pods\n  ##\n  podAnnotations: {}\n\n  ## Labels to be added to pushgateway pods\n  ##\n  podLabels: {}\n\n  ## Specify if a Pod Security Policy for node-exporter must be created\n  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  ##\n  podSecurityPolicy:\n    annotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  replicaCount: 1\n\n  ## Annotations to be added to deployment\n  ##\n  deploymentAnnotations: {}\n\n  ## PodDisruptionBudget settings\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n  ##\n  podDisruptionBudget:\n    enabled: false\n    maxUnavailable: 1\n\n  ## pushgateway resource requests and limits\n  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n    # limits:\n    #   cpu: 10m\n    #   memory: 32Mi\n    # requests:\n    #   cpu: 10m\n    #   memory: 32Mi\n\n  ## Vertical Pod Autoscaler config\n  ## Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler\n  verticalAutoscaler:\n    ## If true a VPA object will be created for the controller\n    enabled: false\n    # updateMode: \"Auto\"\n    # containerPolicies:\n    # - containerName: 'prometheus-pushgateway'\n\n  # Custom DNS configuration to be added to push-gateway pods\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n\n  ## Security context to be added to push-gateway pods\n  ##\n  securityContext:\n    runAsUser: 65534\n    runAsNonRoot: true\n\n  service:\n    annotations:\n      prometheus.io/probe: pushgateway\n    labels: {}\n    clusterIP: \"\"\n\n    ## List of IP addresses at which the pushgateway service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 9091\n    type: ClusterIP\n\n  ## pushgateway Deployment Strategy type\n  # strategy:\n  #   type: Recreate\n\n  persistentVolume:\n    ## If true, pushgateway will create/use a Persistent Volume Claim\n    ##\n    enabled: false\n\n    ## pushgateway data Persistent Volume access modes\n    ## Must match those of existing PV or dynamic provisioner\n    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    ##\n    accessModes:\n      - ReadWriteOnce\n\n    ## pushgateway data Persistent Volume Claim annotations\n    ##\n    annotations: {}\n\n    ## pushgateway data Persistent Volume existing claim name\n    ## Requires pushgateway.persistentVolume.enabled: true\n    ## If defined, PVC must be created manually before volume will be bound\n    existingClaim: \"\"\n\n    ## pushgateway data Persistent Volume mount root path\n    ##\n    mountPath: /data\n\n    ## pushgateway data Persistent Volume size\n    ##\n    size: 2Gi\n\n    ## pushgateway data Persistent Volume Storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    # storageClass: \"-\"\n\n    ## pushgateway data Persistent Volume Binding Mode\n    ## If defined, volumeBindingMode: \u003cvolumeBindingMode\u003e\n    ## If undefined (the default) or set to null, no volumeBindingMode spec is\n    ##   set, choosing the default mode.\n    ##\n    # volumeBindingMode: \"\"\n\n    ## Subdirectory of pushgateway data Persistent Volume to mount\n    ## Useful if the volume's root directory is not empty\n    ##\n    subPath: \"\"\n\n\n## alertmanager ConfigMap entries\n##\nalertmanagerFiles:\n  alertmanager.yml:\n    global: {}\n      # slack_api_url: ''\n\n    receivers:\n      - name: default-receiver\n        # slack_configs:\n        #  - channel: '@you'\n        #    send_resolved: true\n\n    route:\n      group_wait: 10s\n      group_interval: 5m\n      receiver: default-receiver\n      repeat_interval: 3h\n\n## Prometheus server ConfigMap entries\n##\nserverFiles:\n\n  ## Alerts configuration\n  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/\n  alerting_rules.yml:\n    groups:\n    - name: demo alert\n      rules:\n      - alert: High Pod Memory\n        expr: sum(container_memory_usage_bytes) \u003e 10\n        for: 1m\n        labels:\n          severity: slack\n        annotations:\n          summary: High Memory Usage\n  # groups:\n  #   - name: Instances\n  #     rules:\n  #       - alert: InstanceDown\n  #         expr: up == 0\n  #         for: 5m\n  #         labels:\n  #           severity: page\n  #         annotations:\n  #           description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'\n  #           summary: 'Instance {{ $labels.instance }} down'\n  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use alerting_rules.yml\n  alerts: {}\n\n  ## Records configuration\n  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/\n  recording_rules.yml: {}\n  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use recording_rules.yml\n  rules: {}\n\n  prometheus.yml:\n    rule_files:\n      - /etc/config/recording_rules.yml\n      - /etc/config/alerting_rules.yml\n    ## Below two files are DEPRECATED will be removed from this default values file\n      - /etc/config/rules\n      - /etc/config/alerts\n\n    scrape_configs:\n      - job_name: prometheus\n        static_configs:\n          - targets:\n            - localhost:9090\n\n      # A scrape configuration for running Prometheus on a Kubernetes cluster.\n      # This uses separate scrape configs for cluster components (i.e. API server, node)\n      # and services to allow each to use different authentication configs.\n      #\n      # Kubernetes labels will be added as Prometheus labels on metrics via the\n      # `labelmap` relabeling action.\n\n      # Scrape config for API servers.\n      #\n      # Kubernetes exposes API servers as endpoints to the default/kubernetes\n      # service so this uses `endpoints` role and uses relabelling to only keep\n      # the endpoints associated with the default/kubernetes service using the\n      # default named port `https`. This works for single API server deployments as\n      # well as HA API server deployments.\n      - job_name: 'kubernetes-apiservers'\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        # Keep only the default/kubernetes service endpoints for the https port. This\n        # will add targets for each API server which Kubernetes adds an endpoint to\n        # the default/kubernetes service.\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n            action: keep\n            regex: default;kubernetes;https\n\n      - job_name: 'kubernetes-nodes'\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        kubernetes_sd_configs:\n          - role: node\n\n        relabel_configs:\n          - action: labelmap\n            regex: __meta_kubernetes_node_label_(.+)\n          - target_label: __address__\n            replacement: kubernetes.default.svc:443\n          - source_labels: [__meta_kubernetes_node_name]\n            regex: (.+)\n            target_label: __metrics_path__\n            replacement: /api/v1/nodes/$1/proxy/metrics\n\n\n      - job_name: 'kubernetes-nodes-cadvisor'\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        kubernetes_sd_configs:\n          - role: node\n\n        # This configuration will work only on kubelet 1.7.3+\n        # As the scrape endpoints for cAdvisor have changed\n        # if you are using older version you need to change the replacement to\n        # replacement: /api/v1/nodes/$1:4194/proxy/metrics\n        # more info here https://github.com/coreos/prometheus-operator/issues/633\n        relabel_configs:\n          - action: labelmap\n            regex: __meta_kubernetes_node_label_(.+)\n          - target_label: __address__\n            replacement: kubernetes.default.svc:443\n          - source_labels: [__meta_kubernetes_node_name]\n            regex: (.+)\n            target_label: __metrics_path__\n            replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor\n\n      # Scrape config for service endpoints.\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/scrape`: Only scrape services that have a value of\n      # `true`, except if `prometheus.io/scrape-slow` is set to `true` as well.\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: If the metrics are exposed on a different port to the\n      # service then set this appropriately.\n      # * `prometheus.io/param_\u003cparameter\u003e`: If the metrics endpoint uses parameters\n      # then you can set any parameter\n      - job_name: 'kubernetes-service-endpoints'\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]\n            action: drop\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n            action: replace\n            target_label: __scheme__\n            regex: (https?)\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n            action: replace\n            target_label: __address__\n            regex: ([^:]+)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n          - action: labelmap\n            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            action: replace\n            target_label: service\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n      # Scrape config for slow service endpoints; same as above, but with a larger\n      # timeout and a larger interval\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: If the metrics are exposed on a different port to the\n      # service then set this appropriately.\n      # * `prometheus.io/param_\u003cparameter\u003e`: If the metrics endpoint uses parameters\n      # then you can set any parameter\n      - job_name: 'kubernetes-service-endpoints-slow'\n\n        scrape_interval: 5m\n        scrape_timeout: 30s\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n            action: replace\n            target_label: __scheme__\n            regex: (https?)\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n            action: replace\n            target_label: __address__\n            regex: ([^:]+)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n          - action: labelmap\n            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            action: replace\n            target_label: service\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n      - job_name: 'prometheus-pushgateway'\n        honor_labels: true\n\n        kubernetes_sd_configs:\n          - role: service\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n            action: keep\n            regex: pushgateway\n\n      # Example scrape config for probing services via the Blackbox Exporter.\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/probe`: Only probe services that have a value of `true`\n      - job_name: 'kubernetes-services'\n\n        metrics_path: /probe\n        params:\n          module: [http_2xx]\n\n        kubernetes_sd_configs:\n          - role: service\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n            action: keep\n            regex: true\n          - source_labels: [__address__]\n            target_label: __param_target\n          - target_label: __address__\n            replacement: blackbox\n          - source_labels: [__param_target]\n            target_label: instance\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            target_label: service\n\n      # Example scrape config for pods\n      #\n      # The relabeling allows the actual pod scrape endpoint to be configured via the\n      # following annotations:\n      #\n      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`,\n      # except if `prometheus.io/scrape-slow` is set to `true` as well.\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.\n      - job_name: 'kubernetes-pods'\n\n        kubernetes_sd_configs:\n          - role: pod\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]\n            action: drop\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]\n            action: replace\n            regex: (https?)\n            target_label: __scheme__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n            action: replace\n            regex: ([^:]+)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n            target_label: __address__\n          - action: labelmap\n            regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_pod_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_pod_name]\n            action: replace\n            target_label: pod\n          - source_labels: [__meta_kubernetes_pod_phase]\n            regex: Pending|Succeeded|Failed|Completed\n            action: drop\n\n      # Example Scrape config for pods which should be scraped slower. An useful example\n      # would be stackriver-exporter which queries an API on every scrape of the pod\n      #\n      # The relabeling allows the actual pod scrape endpoint to be configured via the\n      # following annotations:\n      #\n      # * `prometheus.io/scrape-slow`: Only scrape pods that have a value of `true`\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.\n      - job_name: 'kubernetes-pods-slow'\n\n        scrape_interval: 5m\n        scrape_timeout: 30s\n\n        kubernetes_sd_configs:\n          - role: pod\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]\n            action: replace\n            regex: (https?)\n            target_label: __scheme__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n            action: replace\n            regex: ([^:]+)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n            target_label: __address__\n          - action: labelmap\n            regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_pod_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_pod_name]\n            action: replace\n            target_label: pod\n          - source_labels: [__meta_kubernetes_pod_phase]\n            regex: Pending|Succeeded|Failed|Completed\n            action: drop\n\n# adds additional scrape configs to prometheus.yml\n# must be a string so you have to add a | after extraScrapeConfigs:\n# example adds prometheus-blackbox-exporter scrape config\nextraScrapeConfigs:\n  # - job_name: 'prometheus-blackbox-exporter'\n  #   metrics_path: /probe\n  #   params:\n  #     module: [http_2xx]\n  #   static_configs:\n  #     - targets:\n  #       - https://example.com\n  #   relabel_configs:\n  #     - source_labels: [__address__]\n  #       target_label: __param_target\n  #     - source_labels: [__param_target]\n  #       target_label: instance\n  #     - target_label: __address__\n  #       replacement: prometheus-blackbox-exporter:9115\n\n# Adds option to add alert_relabel_configs to avoid duplicate alerts in alertmanager\n# useful in H/A prometheus with different external labels but the same alerts\nalertRelabelConfigs:\n  # alert_relabel_configs:\n  # - source_labels: [dc]\n  #   regex: (.+)\\d+\n  #   target_label: dc\n\nnetworkPolicy:\n  ## Enable creation of NetworkPolicy resources.\n  ##\n  enabled: false\n\n# Force namespace of namespaced resources\nforceNamespace: null\n",
            "vars": null
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "consul",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "consul",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "vault-backend",
            "keyring": null,
            "lint": false,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.9.4",
                "chart": "consul",
                "name": "vault-backend",
                "namespace": "staging",
                "revision": 1,
                "values": "{\"server\":{\"bootstrapExpect\":1,\"replicas\":1}}",
                "version": "0.31.1"
              }
            ],
            "name": "vault-backend",
            "namespace": "staging",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://helm.releases.hashicorp.com",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "server.bootstrapExpect",
                "type": "",
                "value": "1"
              },
              {
                "name": "server.replicas",
                "type": "",
                "value": "1"
              }
            ],
            "set_sensitive": [],
            "set_string": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": null,
            "verify": false,
            "version": "0.31.1",
            "wait": true
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "kubernetes_namespace.this",
            "kubernetes_secret.gossip_key",
            "random_id.gossip_key"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "prometheus",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "prometheus",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "prometheus",
            "keyring": null,
            "lint": false,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "2.31.1",
                "chart": "prometheus",
                "name": "prometheus",
                "namespace": "staging",
                "revision": 1,
                "values": "{\"alertRelabelConfigs\":null,\"alertmanager\":{\"affinity\":{},\"baseURL\":\"http://localhost:9093\",\"clusterPeers\":[],\"configFileName\":\"alertmanager.yml\",\"configFromSecret\":\"\",\"configMapOverrideName\":\"\",\"deploymentAnnotations\":{},\"dnsConfig\":{},\"emptyDir\":{\"sizeLimit\":\"\"},\"enabled\":true,\"extraArgs\":{},\"extraConfigmapMounts\":[],\"extraEnv\":{},\"extraInitContainers\":[],\"extraSecretMounts\":[],\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"quay.io/prometheus/alertmanager\",\"tag\":\"v0.23.0\"},\"ingress\":{\"annotations\":{},\"enabled\":false,\"extraLabels\":{},\"extraPaths\":[],\"hosts\":[],\"path\":\"/\",\"pathType\":\"Prefix\",\"tls\":[]},\"name\":\"alertmanager\",\"nodeSelector\":{},\"persistentVolume\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enabled\":true,\"existingClaim\":\"\",\"mountPath\":\"/data\",\"size\":\"2Gi\",\"subPath\":\"\"},\"podAnnotations\":{},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":1},\"podLabels\":{},\"podSecurityPolicy\":{\"annotations\":{}},\"prefixURL\":\"\",\"priorityClassName\":\"\",\"replicaCount\":1,\"resources\":{},\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534},\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"servicePort\":80,\"sessionAffinity\":\"None\",\"type\":\"ClusterIP\"},\"statefulSet\":{\"annotations\":{},\"enabled\":false,\"headless\":{\"annotations\":{},\"enableMeshPeer\":false,\"labels\":{},\"servicePort\":80},\"labels\":{},\"podManagementPolicy\":\"OrderedReady\"},\"tolerations\":[],\"useClusterRole\":true,\"useExistingRole\":false},\"alertmanagerFiles\":{\"alertmanager.yml\":{\"global\":{},\"receivers\":[{\"name\":\"default-receiver\"}],\"route\":{\"group_interval\":\"5m\",\"group_wait\":\"10s\",\"receiver\":\"default-receiver\",\"repeat_interval\":\"3h\"}}},\"configmapReload\":{\"alertmanager\":{\"enabled\":true,\"extraArgs\":{},\"extraConfigmapMounts\":[],\"extraVolumeDirs\":[],\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"jimmidyson/configmap-reload\",\"tag\":\"v0.5.0\"},\"name\":\"configmap-reload\",\"resources\":{}},\"prometheus\":{\"enabled\":true,\"extraArgs\":{},\"extraConfigmapMounts\":[],\"extraVolumeDirs\":[],\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"jimmidyson/configmap-reload\",\"tag\":\"v0.5.0\"},\"name\":\"configmap-reload\",\"resources\":{}}},\"extraScrapeConfigs\":null,\"forceNamespace\":null,\"imagePullSecrets\":null,\"kubeStateMetrics\":{\"enabled\":true},\"networkPolicy\":{\"enabled\":false},\"nodeExporter\":{\"dnsConfig\":{},\"enabled\":true,\"extraArgs\":{},\"extraConfigmapMounts\":[],\"extraHostPathMounts\":[],\"extraInitContainers\":[],\"hostNetwork\":true,\"hostPID\":true,\"hostRootfs\":true,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"quay.io/prometheus/node-exporter\",\"tag\":\"v1.3.0\"},\"name\":\"node-exporter\",\"nodeSelector\":{},\"pod\":{\"labels\":{}},\"podAnnotations\":{},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":1},\"podSecurityPolicy\":{\"annotations\":{}},\"priorityClassName\":\"\",\"resources\":{},\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534},\"service\":{\"annotations\":{\"prometheus.io/scrape\":\"true\"},\"clusterIP\":\"None\",\"externalIPs\":[],\"hostPort\":9100,\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"servicePort\":9100,\"type\":\"ClusterIP\"},\"tolerations\":[],\"updateStrategy\":{\"type\":\"RollingUpdate\"}},\"podSecurityPolicy\":{\"enabled\":false},\"pushgateway\":{\"deploymentAnnotations\":{},\"dnsConfig\":{},\"enabled\":true,\"extraArgs\":{},\"extraInitContainers\":[],\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"prom/pushgateway\",\"tag\":\"v1.4.2\"},\"ingress\":{\"annotations\":{},\"enabled\":false,\"extraPaths\":[],\"hosts\":[],\"path\":\"/\",\"pathType\":\"Prefix\",\"tls\":[]},\"name\":\"pushgateway\",\"nodeSelector\":{},\"persistentVolume\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enabled\":false,\"existingClaim\":\"\",\"mountPath\":\"/data\",\"size\":\"2Gi\",\"subPath\":\"\"},\"podAnnotations\":{},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":1},\"podLabels\":{},\"podSecurityPolicy\":{\"annotations\":{}},\"priorityClassName\":\"\",\"replicaCount\":1,\"resources\":{},\"securityContext\":{\"runAsNonRoot\":true,\"runAsUser\":65534},\"service\":{\"annotations\":{\"prometheus.io/probe\":\"pushgateway\"},\"clusterIP\":\"\",\"externalIPs\":[],\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"servicePort\":9091,\"type\":\"ClusterIP\"},\"tolerations\":[],\"verticalAutoscaler\":{\"enabled\":false}},\"rbac\":{\"create\":true},\"server\":{\"affinity\":{},\"alertmanagers\":[],\"baseURL\":\"\",\"configMapOverrideName\":\"\",\"configPath\":\"/etc/config/prometheus.yml\",\"deploymentAnnotations\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"emptyDir\":{\"sizeLimit\":\"\"},\"enableServiceLinks\":true,\"enabled\":true,\"env\":[],\"extraArgs\":{},\"extraConfigmapMounts\":[],\"extraFlags\":[\"web.enable-lifecycle\"],\"extraHostPathMounts\":[],\"extraInitContainers\":[],\"extraSecretMounts\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"global\":{\"evaluation_interval\":\"1m\",\"scrape_interval\":\"1m\",\"scrape_timeout\":\"10s\"},\"hostAliases\":[],\"hostNetwork\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"quay.io/prometheus/prometheus\",\"tag\":\"v2.31.1\"},\"ingress\":{\"annotations\":{},\"enabled\":false,\"extraLabels\":{},\"extraPaths\":[],\"hosts\":[],\"path\":\"/\",\"pathType\":\"Prefix\",\"tls\":[]},\"livenessProbeFailureThreshold\":3,\"livenessProbeInitialDelay\":30,\"livenessProbePeriodSeconds\":15,\"livenessProbeSuccessThreshold\":1,\"livenessProbeTimeout\":10,\"name\":\"server\",\"nodeSelector\":{},\"persistentVolume\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enabled\":true,\"existingClaim\":\"\",\"mountPath\":\"/data\",\"size\":\"8Gi\",\"subPath\":\"\"},\"podAnnotations\":{},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":1},\"podLabels\":{},\"podSecurityPolicy\":{\"annotations\":{}},\"prefixURL\":\"\",\"priorityClassName\":\"\",\"probeHeaders\":[],\"probeScheme\":\"HTTP\",\"readinessProbeFailureThreshold\":3,\"readinessProbeInitialDelay\":30,\"readinessProbePeriodSeconds\":5,\"readinessProbeSuccessThreshold\":1,\"readinessProbeTimeout\":4,\"remoteRead\":[],\"remoteWrite\":[],\"replicaCount\":1,\"resources\":{},\"retention\":\"15d\",\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534},\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"gRPC\":{\"enabled\":false,\"servicePort\":10901},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"servicePort\":80,\"sessionAffinity\":\"None\",\"statefulsetReplica\":{\"enabled\":false,\"replica\":0},\"type\":\"ClusterIP\"},\"sidecarContainers\":{},\"sidecarTemplateValues\":{},\"startupProbe\":{\"enabled\":false,\"failureThreshold\":30,\"periodSeconds\":5,\"timeoutSeconds\":10},\"statefulSet\":{\"annotations\":{},\"enabled\":false,\"headless\":{\"annotations\":{},\"gRPC\":{\"enabled\":false,\"servicePort\":10901},\"labels\":{},\"servicePort\":80},\"labels\":{},\"podManagementPolicy\":\"OrderedReady\"},\"storagePath\":\"\",\"tcpSocketProbeEnabled\":false,\"terminationGracePeriodSeconds\":300,\"tolerations\":[],\"verticalAutoscaler\":{\"enabled\":false}},\"serverFiles\":{\"alerting_rules.yml\":{\"groups\":[{\"name\":\"demo alert\",\"rules\":[{\"alert\":\"High Pod Memory\",\"annotations\":{\"summary\":\"High Memory Usage\"},\"expr\":\"sum(container_memory_usage_bytes) \\u003e 10\",\"for\":\"1m\",\"labels\":{\"severity\":\"slack\"}}]}]},\"alerts\":{},\"prometheus.yml\":{\"rule_files\":[\"/etc/config/recording_rules.yml\",\"/etc/config/alerting_rules.yml\",\"/etc/config/rules\",\"/etc/config/alerts\"],\"scrape_configs\":[{\"job_name\":\"prometheus\",\"static_configs\":[{\"targets\":[\"localhost:9090\"]}]},{\"bearer_token_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/token\",\"job_name\":\"kubernetes-apiservers\",\"kubernetes_sd_configs\":[{\"role\":\"endpoints\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":\"default;kubernetes;https\",\"source_labels\":[\"__meta_kubernetes_namespace\",\"__meta_kubernetes_service_name\",\"__meta_kubernetes_endpoint_port_name\"]}],\"scheme\":\"https\",\"tls_config\":{\"ca_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\"insecure_skip_verify\":true}},{\"bearer_token_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/token\",\"job_name\":\"kubernetes-nodes\",\"kubernetes_sd_configs\":[{\"role\":\"node\"}],\"relabel_configs\":[{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_node_label_(.+)\"},{\"replacement\":\"kubernetes.default.svc:443\",\"target_label\":\"__address__\"},{\"regex\":\"(.+)\",\"replacement\":\"/api/v1/nodes/$1/proxy/metrics\",\"source_labels\":[\"__meta_kubernetes_node_name\"],\"target_label\":\"__metrics_path__\"}],\"scheme\":\"https\",\"tls_config\":{\"ca_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\"insecure_skip_verify\":true}},{\"bearer_token_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/token\",\"job_name\":\"kubernetes-nodes-cadvisor\",\"kubernetes_sd_configs\":[{\"role\":\"node\"}],\"relabel_configs\":[{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_node_label_(.+)\"},{\"replacement\":\"kubernetes.default.svc:443\",\"target_label\":\"__address__\"},{\"regex\":\"(.+)\",\"replacement\":\"/api/v1/nodes/$1/proxy/metrics/cadvisor\",\"source_labels\":[\"__meta_kubernetes_node_name\"],\"target_label\":\"__metrics_path__\"}],\"scheme\":\"https\",\"tls_config\":{\"ca_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\"insecure_skip_verify\":true}},{\"job_name\":\"kubernetes-service-endpoints\",\"kubernetes_sd_configs\":[{\"role\":\"endpoints\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scrape\"]},{\"action\":\"drop\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"([^:]+)(?::\\\\d+)?;(\\\\d+)\",\"replacement\":\"$1:$2\",\"source_labels\":[\"__address__\",\"__meta_kubernetes_service_annotation_prometheus_io_port\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_service_name\"],\"target_label\":\"service\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_node_name\"],\"target_label\":\"node\"}]},{\"job_name\":\"kubernetes-service-endpoints-slow\",\"kubernetes_sd_configs\":[{\"role\":\"endpoints\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"([^:]+)(?::\\\\d+)?;(\\\\d+)\",\"replacement\":\"$1:$2\",\"source_labels\":[\"__address__\",\"__meta_kubernetes_service_annotation_prometheus_io_port\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_service_name\"],\"target_label\":\"service\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_node_name\"],\"target_label\":\"node\"}],\"scrape_interval\":\"5m\",\"scrape_timeout\":\"30s\"},{\"honor_labels\":true,\"job_name\":\"prometheus-pushgateway\",\"kubernetes_sd_configs\":[{\"role\":\"service\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":\"pushgateway\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_probe\"]}]},{\"job_name\":\"kubernetes-services\",\"kubernetes_sd_configs\":[{\"role\":\"service\"}],\"metrics_path\":\"/probe\",\"params\":{\"module\":[\"http_2xx\"]},\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_probe\"]},{\"source_labels\":[\"__address__\"],\"target_label\":\"__param_target\"},{\"replacement\":\"blackbox\",\"target_label\":\"__address__\"},{\"source_labels\":[\"__param_target\"],\"target_label\":\"instance\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_label_(.+)\"},{\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"source_labels\":[\"__meta_kubernetes_service_name\"],\"target_label\":\"service\"}]},{\"job_name\":\"kubernetes-pods\",\"kubernetes_sd_configs\":[{\"role\":\"pod\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scrape\"]},{\"action\":\"drop\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"([^:]+)(?::\\\\d+)?;(\\\\d+)\",\"replacement\":\"$1:$2\",\"source_labels\":[\"__address__\",\"__meta_kubernetes_pod_annotation_prometheus_io_port\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_name\"],\"target_label\":\"pod\"},{\"action\":\"drop\",\"regex\":\"Pending|Succeeded|Failed|Completed\",\"source_labels\":[\"__meta_kubernetes_pod_phase\"]}]},{\"job_name\":\"kubernetes-pods-slow\",\"kubernetes_sd_configs\":[{\"role\":\"pod\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"([^:]+)(?::\\\\d+)?;(\\\\d+)\",\"replacement\":\"$1:$2\",\"source_labels\":[\"__address__\",\"__meta_kubernetes_pod_annotation_prometheus_io_port\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_name\"],\"target_label\":\"pod\"},{\"action\":\"drop\",\"regex\":\"Pending|Succeeded|Failed|Completed\",\"source_labels\":[\"__meta_kubernetes_pod_phase\"]}],\"scrape_interval\":\"5m\",\"scrape_timeout\":\"30s\"}]},\"recording_rules.yml\":{},\"rules\":{}},\"serviceAccounts\":{\"alertmanager\":{\"annotations\":{},\"create\":true,\"name\":null},\"nodeExporter\":{\"annotations\":{},\"create\":true,\"name\":null},\"pushgateway\":{\"annotations\":{},\"create\":true,\"name\":null},\"server\":{\"annotations\":{},\"create\":true,\"name\":null}}}",
                "version": "15.1.3"
              }
            ],
            "name": "prometheus",
            "namespace": "staging",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://prometheus-community.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "set_string": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "rbac:\n  create: true\n\npodSecurityPolicy:\n  enabled: false\n\nimagePullSecrets:\n# - name: \"image-pull-secret\"\n\n## Define serviceAccount names for components. Defaults to component's fully qualified name.\n##\nserviceAccounts:\n  alertmanager:\n    create: true\n    name:\n    annotations: {}\n  nodeExporter:\n    create: true\n    name:\n    annotations: {}\n  pushgateway:\n    create: true\n    name:\n    annotations: {}\n  server:\n    create: true\n    name:\n    annotations: {}\n\nalertmanager:\n  ## If false, alertmanager will not be installed\n  ##\n  enabled: true\n\n  ## Use a ClusterRole (and ClusterRoleBinding)\n  ## - If set to false - we define a Role and RoleBinding in the defined namespaces ONLY\n  ## This makes alertmanager work - for users who do not have ClusterAdmin privs, but wants alertmanager to operate on their own namespaces, instead of clusterwide.\n  useClusterRole: true\n\n  ## Set to a rolename to use existing role - skipping role creating - but still doing serviceaccount and rolebinding to the rolename set here.\n  useExistingRole: false\n\n  ## alertmanager container name\n  ##\n  name: alertmanager\n\n  ## alertmanager container image\n  ##\n  image:\n    repository: quay.io/prometheus/alertmanager\n    tag: v0.23.0\n    pullPolicy: IfNotPresent\n\n  ## alertmanager priorityClassName\n  ##\n  priorityClassName: \"\"\n\n  ## Additional alertmanager container arguments\n  ##\n  extraArgs: {}\n\n  ## Additional InitContainers to initialize the pod\n  ##\n  extraInitContainers: []\n\n  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug\n  ## so that the various internal URLs are still able to access as they are in the default case.\n  ## (Optional)\n  prefixURL: \"\"\n\n  ## External URL which can access alertmanager\n  baseURL: \"http://localhost:9093\"\n\n  ## Additional alertmanager container environment variable\n  ## For instance to add a http_proxy\n  ##\n  extraEnv: {}\n\n  ## Additional alertmanager Secret mounts\n  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.\n  extraSecretMounts: []\n    # - name: secret-files\n    #   mountPath: /etc/secrets\n    #   subPath: \"\"\n    #   secretName: alertmanager-secret-files\n    #   readOnly: true\n\n  ## Additional alertmanager Configmap mounts\n  extraConfigmapMounts: []\n    # - name: template-files\n    #   mountPath: /etc/config/templates.d\n    #   configMap: alertmanager-template-files\n    #   readOnly: true\n\n  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.alertmanager.configMapOverrideName}}\n  ## Defining configMapOverrideName will cause templates/alertmanager-configmap.yaml\n  ## to NOT generate a ConfigMap resource\n  ##\n  configMapOverrideName: \"\"\n\n  ## The name of a secret in the same kubernetes namespace which contains the Alertmanager config\n  ## Defining configFromSecret will cause templates/alertmanager-configmap.yaml\n  ## to NOT generate a ConfigMap resource\n  ##\n  configFromSecret: \"\"\n\n  ## The configuration file name to be loaded to alertmanager\n  ## Must match the key within configuration loaded from ConfigMap/Secret\n  ##\n  configFileName: alertmanager.yml\n\n  ingress:\n    ## If true, alertmanager Ingress will be created\n    ##\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    ## alertmanager Ingress annotations\n    ##\n    annotations: {}\n    #   kubernetes.io/ingress.class: nginx\n    #   kubernetes.io/tls-acme: 'true'\n\n    ## alertmanager Ingress additional labels\n    ##\n    extraLabels: {}\n\n    ## alertmanager Ingress hostnames with optional path\n    ## Must be provided if Ingress is enabled\n    ##\n    hosts: []\n    #   - alertmanager.domain.com\n    #   - domain.com/alertmanager\n\n    path: /\n\n    # pathType is only for k8s \u003e= 1.18\n    pathType: Prefix\n\n    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.\n    extraPaths: []\n    # - path: /*\n    #   backend:\n    #     serviceName: ssl-redirect\n    #     servicePort: use-annotation\n\n    ## alertmanager Ingress TLS configuration\n    ## Secrets must be manually created in the namespace\n    ##\n    tls: []\n    #   - secretName: prometheus-alerts-tls\n    #     hosts:\n    #       - alertmanager.domain.com\n\n  ## Alertmanager Deployment Strategy type\n  # strategy:\n  #   type: Recreate\n\n  ## Node tolerations for alertmanager scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal|Exists\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  ## Node labels for alertmanager pod assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Pod affinity\n  ##\n  affinity: {}\n\n  ## PodDisruptionBudget settings\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n  ##\n  podDisruptionBudget:\n    enabled: false\n    maxUnavailable: 1\n\n  ## Use an alternate scheduler, e.g. \"stork\".\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  # schedulerName:\n\n  persistentVolume:\n    ## If true, alertmanager will create/use a Persistent Volume Claim\n    ## If false, use emptyDir\n    ##\n    enabled: true\n\n    ## alertmanager data Persistent Volume access modes\n    ## Must match those of existing PV or dynamic provisioner\n    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    ##\n    accessModes:\n      - ReadWriteOnce\n\n    ## alertmanager data Persistent Volume Claim annotations\n    ##\n    annotations: {}\n\n    ## alertmanager data Persistent Volume existing claim name\n    ## Requires alertmanager.persistentVolume.enabled: true\n    ## If defined, PVC must be created manually before volume will be bound\n    existingClaim: \"\"\n\n    ## alertmanager data Persistent Volume mount root path\n    ##\n    mountPath: /data\n\n    ## alertmanager data Persistent Volume size\n    ##\n    size: 2Gi\n\n    ## alertmanager data Persistent Volume Storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    # storageClass: \"-\"\n\n    ## alertmanager data Persistent Volume Binding Mode\n    ## If defined, volumeBindingMode: \u003cvolumeBindingMode\u003e\n    ## If undefined (the default) or set to null, no volumeBindingMode spec is\n    ##   set, choosing the default mode.\n    ##\n    # volumeBindingMode: \"\"\n\n    ## Subdirectory of alertmanager data Persistent Volume to mount\n    ## Useful if the volume's root directory is not empty\n    ##\n    subPath: \"\"\n\n    ## Persistent Volume Claim Selector\n    ## Useful if Persistent Volumes have been provisioned in advance\n    ## Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector\n    ##\n    # selector:\n    #  matchLabels:\n    #    release: \"stable\"\n    #  matchExpressions:\n    #    - { key: environment, operator: In, values: [ dev ] }\n\n  emptyDir:\n    ## alertmanager emptyDir volume size limit\n    ##\n    sizeLimit: \"\"\n\n  ## Annotations to be added to alertmanager pods\n  ##\n  podAnnotations: {}\n    ## Tell prometheus to use a specific set of alertmanager pods\n    ## instead of all alertmanager pods found in the same namespace\n    ## Useful if you deploy multiple releases within the same namespace\n    ##\n    ## prometheus.io/probe: alertmanager-teamA\n\n  ## Labels to be added to Prometheus AlertManager pods\n  ##\n  podLabels: {}\n\n  ## Specify if a Pod Security Policy for node-exporter must be created\n  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  ##\n  podSecurityPolicy:\n    annotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)\n  ##\n  replicaCount: 1\n\n  ## Annotations to be added to deployment\n  ##\n  deploymentAnnotations: {}\n\n  statefulSet:\n    ## If true, use a statefulset instead of a deployment for pod management.\n    ## This allows to scale replicas to more than 1 pod\n    ##\n    enabled: false\n\n    annotations: {}\n    labels: {}\n    podManagementPolicy: OrderedReady\n\n    ## Alertmanager headless service to use for the statefulset\n    ##\n    headless:\n      annotations: {}\n      labels: {}\n\n      ## Enabling peer mesh service end points for enabling the HA alert manager\n      ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md\n      enableMeshPeer: false\n\n      servicePort: 80\n\n  ## alertmanager resource requests and limits\n  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n    # limits:\n    #   cpu: 10m\n    #   memory: 32Mi\n    # requests:\n    #   cpu: 10m\n    #   memory: 32Mi\n\n  # Custom DNS configuration to be added to alertmanager pods\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n\n  ## Security context to be added to alertmanager pods\n  ##\n  securityContext:\n    runAsUser: 65534\n    runAsNonRoot: true\n    runAsGroup: 65534\n    fsGroup: 65534\n\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n    ## Enabling peer mesh service end points for enabling the HA alert manager\n    ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md\n    # enableMeshPeer : true\n\n    ## List of IP addresses at which the alertmanager service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 80\n    # nodePort: 30000\n    sessionAffinity: None\n    type: ClusterIP\n\n  ## List of initial peers\n  ## Ref: https://github.com/prometheus/alertmanager/blob/main/README.md#high-availability\n  clusterPeers: []\n\n## Monitors ConfigMap changes and POSTs to a URL\n## Ref: https://github.com/jimmidyson/configmap-reload\n##\nconfigmapReload:\n  prometheus:\n    ## If false, the configmap-reload container will not be deployed\n    ##\n    enabled: true\n\n    ## configmap-reload container name\n    ##\n    name: configmap-reload\n\n    ## configmap-reload container image\n    ##\n    image:\n      repository: jimmidyson/configmap-reload\n      tag: v0.5.0\n      pullPolicy: IfNotPresent\n\n    ## Additional configmap-reload container arguments\n    ##\n    extraArgs: {}\n    ## Additional configmap-reload volume directories\n    ##\n    extraVolumeDirs: []\n\n\n    ## Additional configmap-reload mounts\n    ##\n    extraConfigmapMounts: []\n      # - name: prometheus-alerts\n      #   mountPath: /etc/alerts.d\n      #   subPath: \"\"\n      #   configMap: prometheus-alerts\n      #   readOnly: true\n\n\n    ## configmap-reload resource requests and limits\n    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n  alertmanager:\n    ## If false, the configmap-reload container will not be deployed\n    ##\n    enabled: true\n\n    ## configmap-reload container name\n    ##\n    name: configmap-reload\n\n    ## configmap-reload container image\n    ##\n    image:\n      repository: jimmidyson/configmap-reload\n      tag: v0.5.0\n      pullPolicy: IfNotPresent\n\n    ## Additional configmap-reload container arguments\n    ##\n    extraArgs: {}\n    ## Additional configmap-reload volume directories\n    ##\n    extraVolumeDirs: []\n\n\n    ## Additional configmap-reload mounts\n    ##\n    extraConfigmapMounts: []\n      # - name: prometheus-alerts\n      #   mountPath: /etc/alerts.d\n      #   subPath: \"\"\n      #   configMap: prometheus-alerts\n      #   readOnly: true\n\n\n    ## configmap-reload resource requests and limits\n    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n\nkubeStateMetrics:\n  ## If false, kube-state-metrics sub-chart will not be installed\n  ##\n  enabled: true\n\n## kube-state-metrics sub-chart configurable values\n## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics\n##\n# kube-state-metrics:\n\nnodeExporter:\n  ## If false, node-exporter will not be installed\n  ##\n  enabled: true\n\n  ## If true, node-exporter pods share the host network namespace\n  ##\n  hostNetwork: true\n\n  ## If true, node-exporter pods share the host PID namespace\n  ##\n  hostPID: true\n\n  ## If true, node-exporter pods mounts host / at /host/root\n  ##\n  hostRootfs: true\n\n  ## node-exporter container name\n  ##\n  name: node-exporter\n\n  ## node-exporter container image\n  ##\n  image:\n    repository: quay.io/prometheus/node-exporter\n    tag: v1.3.0\n    pullPolicy: IfNotPresent\n\n  ## Specify if a Pod Security Policy for node-exporter must be created\n  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  ##\n  podSecurityPolicy:\n    annotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## node-exporter priorityClassName\n  ##\n  priorityClassName: \"\"\n\n  ## Custom Update Strategy\n  ##\n  updateStrategy:\n    type: RollingUpdate\n\n  ## Additional node-exporter container arguments\n  ##\n  extraArgs: {}\n\n  ## Additional InitContainers to initialize the pod\n  ##\n  extraInitContainers: []\n\n  ## Additional node-exporter hostPath mounts\n  ##\n  extraHostPathMounts: []\n    # - name: textfile-dir\n    #   mountPath: /srv/txt_collector\n    #   hostPath: /var/lib/node-exporter\n    #   readOnly: true\n    #   mountPropagation: HostToContainer\n\n  extraConfigmapMounts: []\n    # - name: certs-configmap\n    #   mountPath: /prometheus\n    #   configMap: certs-configmap\n    #   readOnly: true\n\n  ## Node tolerations for node-exporter scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal|Exists\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  ## Node labels for node-exporter pod assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Annotations to be added to node-exporter pods\n  ##\n  podAnnotations: {}\n\n  ## Labels to be added to node-exporter pods\n  ##\n  pod:\n    labels: {}\n\n  ## PodDisruptionBudget settings\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n  ##\n  podDisruptionBudget:\n    enabled: false\n    maxUnavailable: 1\n\n  ## node-exporter resource limits \u0026 requests\n  ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n    # limits:\n    #   cpu: 200m\n    #   memory: 50Mi\n    # requests:\n    #   cpu: 100m\n    #   memory: 30Mi\n\n  # Custom DNS configuration to be added to node-exporter pods\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n\n  ## Security context to be added to node-exporter pods\n  ##\n  securityContext:\n    fsGroup: 65534\n    runAsGroup: 65534\n    runAsNonRoot: true\n    runAsUser: 65534\n\n  service:\n    annotations:\n      prometheus.io/scrape: \"true\"\n    labels: {}\n\n    # Exposed as a headless service:\n    # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services\n    clusterIP: None\n\n    ## List of IP addresses at which the node-exporter service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    hostPort: 9100\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 9100\n    type: ClusterIP\n\nserver:\n  ## Prometheus server container name\n  ##\n  enabled: true\n\n  ## Use a ClusterRole (and ClusterRoleBinding)\n  ## - If set to false - we define a RoleBinding in the defined namespaces ONLY\n  ##\n  ## NB: because we need a Role with nonResourceURL's (\"/metrics\") - you must get someone with Cluster-admin privileges to define this role for you, before running with this setting enabled.\n  ##     This makes prometheus work - for users who do not have ClusterAdmin privs, but wants prometheus to operate on their own namespaces, instead of clusterwide.\n  ##\n  ## You MUST also set namespaces to the ones you have access to and want monitored by Prometheus.\n  ##\n  # useExistingClusterRoleName: nameofclusterrole\n\n  ## namespaces to monitor (instead of monitoring all - clusterwide). Needed if you want to run without Cluster-admin privileges.\n  # namespaces:\n  #   - yournamespace\n\n  name: server\n\n  # sidecarContainers - add more containers to prometheus server\n  # Key/Value where Key is the sidecar `- name: \u003cKey\u003e`\n  # Example:\n  #   sidecarContainers:\n  #      webserver:\n  #        image: nginx\n  sidecarContainers: {}\n\n  # sidecarTemplateValues - context to be used in template for sidecarContainers\n  # Example:\n  #   sidecarTemplateValues: *your-custom-globals\n  #   sidecarContainers:\n  #     webserver: |-\n  #       {{ include \"webserver-container-template\" . }}\n  # Template for `webserver-container-template` might looks like this:\n  #   image: \"{{ .Values.server.sidecarTemplateValues.repository }}:{{ .Values.server.sidecarTemplateValues.tag }}\"\n  #   ...\n  #\n  sidecarTemplateValues: {}\n\n  ## Prometheus server container image\n  ##\n  image:\n    repository: quay.io/prometheus/prometheus\n    tag: v2.31.1\n    pullPolicy: IfNotPresent\n\n  ## prometheus server priorityClassName\n  ##\n  priorityClassName: \"\"\n\n  ## EnableServiceLinks indicates whether information about services should be injected\n  ## into pod's environment variables, matching the syntax of Docker links.\n  ## WARNING: the field is unsupported and will be skipped in K8s prior to v1.13.0.\n  ##\n  enableServiceLinks: true\n\n  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug\n  ## so that the various internal URLs are still able to access as they are in the default case.\n  ## (Optional)\n  prefixURL: \"\"\n\n  ## External URL which can access prometheus\n  ## Maybe same with Ingress host name\n  baseURL: \"\"\n\n  ## Additional server container environment variables\n  ##\n  ## You specify this manually like you would a raw deployment manifest.\n  ## This means you can bind in environment variables from secrets.\n  ##\n  ## e.g. static environment variable:\n  ##  - name: DEMO_GREETING\n  ##    value: \"Hello from the environment\"\n  ##\n  ## e.g. secret environment variable:\n  ## - name: USERNAME\n  ##   valueFrom:\n  ##     secretKeyRef:\n  ##       name: mysecret\n  ##       key: username\n  env: []\n\n  extraFlags:\n    - web.enable-lifecycle\n    ## web.enable-admin-api flag controls access to the administrative HTTP API which includes functionality such as\n    ## deleting time series. This is disabled by default.\n    # - web.enable-admin-api\n    ##\n    ## storage.tsdb.no-lockfile flag controls BD locking\n    # - storage.tsdb.no-lockfile\n    ##\n    ## storage.tsdb.wal-compression flag enables compression of the write-ahead log (WAL)\n    # - storage.tsdb.wal-compression\n\n  ## Path to a configuration file on prometheus server container FS\n  configPath: /etc/config/prometheus.yml\n\n  ### The data directory used by prometheus to set --storage.tsdb.path\n  ### When empty server.persistentVolume.mountPath is used instead\n  storagePath: \"\"\n\n  global:\n    ## How frequently to scrape targets by default\n    ##\n    scrape_interval: 1m\n    ## How long until a scrape request times out\n    ##\n    scrape_timeout: 10s\n    ## How frequently to evaluate rules\n    ##\n    evaluation_interval: 1m\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write\n  ##\n  remoteWrite: []\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_read\n  ##\n  remoteRead: []\n\n  ## Custom HTTP headers for Liveness/Readiness/Startup Probe\n  ##\n  ## Useful for providing HTTP Basic Auth to healthchecks\n  probeHeaders: []\n\n  ## Additional Prometheus server container arguments\n  ##\n  extraArgs: {}\n\n  ## Additional InitContainers to initialize the pod\n  ##\n  extraInitContainers: []\n\n  ## Additional Prometheus server Volume mounts\n  ##\n  extraVolumeMounts: []\n\n  ## Additional Prometheus server Volumes\n  ##\n  extraVolumes: []\n\n  ## Additional Prometheus server hostPath mounts\n  ##\n  extraHostPathMounts: []\n    # - name: certs-dir\n    #   mountPath: /etc/kubernetes/certs\n    #   subPath: \"\"\n    #   hostPath: /etc/kubernetes/certs\n    #   readOnly: true\n\n  extraConfigmapMounts: []\n    # - name: certs-configmap\n    #   mountPath: /prometheus\n    #   subPath: \"\"\n    #   configMap: certs-configmap\n    #   readOnly: true\n\n  ## Additional Prometheus server Secret mounts\n  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.\n  extraSecretMounts: []\n    # - name: secret-files\n    #   mountPath: /etc/secrets\n    #   subPath: \"\"\n    #   secretName: prom-secret-files\n    #   readOnly: true\n\n  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}}\n  ## Defining configMapOverrideName will cause templates/server-configmap.yaml\n  ## to NOT generate a ConfigMap resource\n  ##\n  configMapOverrideName: \"\"\n\n  ingress:\n    ## If true, Prometheus server Ingress will be created\n    ##\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    ## Prometheus server Ingress annotations\n    ##\n    annotations: {}\n    #   kubernetes.io/ingress.class: nginx\n    #   kubernetes.io/tls-acme: 'true'\n\n    ## Prometheus server Ingress additional labels\n    ##\n    extraLabels: {}\n\n    ## Prometheus server Ingress hostnames with optional path\n    ## Must be provided if Ingress is enabled\n    ##\n    hosts: []\n    #   - prometheus.domain.com\n    #   - domain.com/prometheus\n\n    path: /\n\n    # pathType is only for k8s \u003e= 1.18\n    pathType: Prefix\n\n    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.\n    extraPaths: []\n    # - path: /*\n    #   backend:\n    #     serviceName: ssl-redirect\n    #     servicePort: use-annotation\n\n    ## Prometheus server Ingress TLS configuration\n    ## Secrets must be manually created in the namespace\n    ##\n    tls: []\n    #   - secretName: prometheus-server-tls\n    #     hosts:\n    #       - prometheus.domain.com\n\n  ## Server Deployment Strategy type\n  # strategy:\n  #   type: Recreate\n\n  ## hostAliases allows adding entries to /etc/hosts inside the containers\n  hostAliases: []\n  #   - ip: \"127.0.0.1\"\n  #     hostnames:\n  #       - \"example.com\"\n\n  ## Node tolerations for server scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal|Exists\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  ## Node labels for Prometheus server pod assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Pod affinity\n  ##\n  affinity: {}\n\n  ## PodDisruptionBudget settings\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n  ##\n  podDisruptionBudget:\n    enabled: false\n    maxUnavailable: 1\n\n  ## Use an alternate scheduler, e.g. \"stork\".\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  # schedulerName:\n\n  persistentVolume:\n    ## If true, Prometheus server will create/use a Persistent Volume Claim\n    ## If false, use emptyDir\n    ##\n    enabled: true\n\n    ## Prometheus server data Persistent Volume access modes\n    ## Must match those of existing PV or dynamic provisioner\n    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    ##\n    accessModes:\n      - ReadWriteOnce\n\n    ## Prometheus server data Persistent Volume annotations\n    ##\n    annotations: {}\n\n    ## Prometheus server data Persistent Volume existing claim name\n    ## Requires server.persistentVolume.enabled: true\n    ## If defined, PVC must be created manually before volume will be bound\n    existingClaim: \"\"\n\n    ## Prometheus server data Persistent Volume mount root path\n    ##\n    mountPath: /data\n\n    ## Prometheus server data Persistent Volume size\n    ##\n    size: 8Gi\n\n    ## Prometheus server data Persistent Volume Storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    # storageClass: \"-\"\n\n    ## Prometheus server data Persistent Volume Binding Mode\n    ## If defined, volumeBindingMode: \u003cvolumeBindingMode\u003e\n    ## If undefined (the default) or set to null, no volumeBindingMode spec is\n    ##   set, choosing the default mode.\n    ##\n    # volumeBindingMode: \"\"\n\n    ## Subdirectory of Prometheus server data Persistent Volume to mount\n    ## Useful if the volume's root directory is not empty\n    ##\n    subPath: \"\"\n\n    ## Persistent Volume Claim Selector\n    ## Useful if Persistent Volumes have been provisioned in advance\n    ## Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector\n    ##\n    # selector:\n    #  matchLabels:\n    #    release: \"stable\"\n    #  matchExpressions:\n    #    - { key: environment, operator: In, values: [ dev ] }\n\n  emptyDir:\n    ## Prometheus server emptyDir volume size limit\n    ##\n    sizeLimit: \"\"\n\n  ## Annotations to be added to Prometheus server pods\n  ##\n  podAnnotations: {}\n    # iam.amazonaws.com/role: prometheus\n\n  ## Labels to be added to Prometheus server pods\n  ##\n  podLabels: {}\n\n  ## Prometheus AlertManager configuration\n  ##\n  alertmanagers: []\n\n  ## Specify if a Pod Security Policy for node-exporter must be created\n  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  ##\n  podSecurityPolicy:\n    annotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)\n  ##\n  replicaCount: 1\n\n  ## Annotations to be added to deployment\n  ##\n  deploymentAnnotations: {}\n\n  statefulSet:\n    ## If true, use a statefulset instead of a deployment for pod management.\n    ## This allows to scale replicas to more than 1 pod\n    ##\n    enabled: false\n\n    annotations: {}\n    labels: {}\n    podManagementPolicy: OrderedReady\n\n    ## Alertmanager headless service to use for the statefulset\n    ##\n    headless:\n      annotations: {}\n      labels: {}\n      servicePort: 80\n      ## Enable gRPC port on service to allow auto discovery with thanos-querier\n      gRPC:\n        enabled: false\n        servicePort: 10901\n        # nodePort: 10901\n\n  ## Prometheus server readiness and liveness probe initial delay and timeout\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n  ##\n  tcpSocketProbeEnabled: false\n  probeScheme: HTTP\n  readinessProbeInitialDelay: 30\n  readinessProbePeriodSeconds: 5\n  readinessProbeTimeout: 4\n  readinessProbeFailureThreshold: 3\n  readinessProbeSuccessThreshold: 1\n  livenessProbeInitialDelay: 30\n  livenessProbePeriodSeconds: 15\n  livenessProbeTimeout: 10\n  livenessProbeFailureThreshold: 3\n  livenessProbeSuccessThreshold: 1\n  startupProbe:\n    enabled: false\n    periodSeconds: 5\n    failureThreshold: 30\n    timeoutSeconds: 10\n\n  ## Prometheus server resource requests and limits\n  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n    # limits:\n    #   cpu: 500m\n    #   memory: 512Mi\n    # requests:\n    #   cpu: 500m\n    #   memory: 512Mi\n\n  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n  ##\n  hostNetwork: false\n\n  # When hostNetwork is enabled, you probably want to set this to ClusterFirstWithHostNet\n  dnsPolicy: ClusterFirst\n\n  ## Vertical Pod Autoscaler config\n  ## Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler\n  verticalAutoscaler:\n    ## If true a VPA object will be created for the controller (either StatefulSet or Deployemnt, based on above configs)\n    enabled: false\n    # updateMode: \"Auto\"\n    # containerPolicies:\n    # - containerName: 'prometheus-server'\n\n  # Custom DNS configuration to be added to prometheus server pods\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n  ## Security context to be added to server pods\n  ##\n  securityContext:\n    runAsUser: 65534\n    runAsNonRoot: true\n    runAsGroup: 65534\n    fsGroup: 65534\n\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 80\n    sessionAffinity: None\n    type: ClusterIP\n\n    ## Enable gRPC port on service to allow auto discovery with thanos-querier\n    gRPC:\n      enabled: false\n      servicePort: 10901\n      # nodePort: 10901\n\n    ## If using a statefulSet (statefulSet.enabled=true), configure the\n    ## service to connect to a specific replica to have a consistent view\n    ## of the data.\n    statefulsetReplica:\n      enabled: false\n      replica: 0\n\n  ## Prometheus server pod termination grace period\n  ##\n  terminationGracePeriodSeconds: 300\n\n  ## Prometheus data retention period (default if not specified is 15 days)\n  ##\n  retention: \"15d\"\n\npushgateway:\n  ## If false, pushgateway will not be installed\n  ##\n  enabled: true\n\n  ## Use an alternate scheduler, e.g. \"stork\".\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  # schedulerName:\n\n  ## pushgateway container name\n  ##\n  name: pushgateway\n\n  ## pushgateway container image\n  ##\n  image:\n    repository: prom/pushgateway\n    tag: v1.4.2\n    pullPolicy: IfNotPresent\n\n  ## pushgateway priorityClassName\n  ##\n  priorityClassName: \"\"\n\n  ## Additional pushgateway container arguments\n  ##\n  ## for example: persistence.file: /data/pushgateway.data\n  extraArgs: {}\n\n  ## Additional InitContainers to initialize the pod\n  ##\n  extraInitContainers: []\n\n  ingress:\n    ## If true, pushgateway Ingress will be created\n    ##\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    ## pushgateway Ingress annotations\n    ##\n    annotations: {}\n    #   kubernetes.io/ingress.class: nginx\n    #   kubernetes.io/tls-acme: 'true'\n\n    ## pushgateway Ingress hostnames with optional path\n    ## Must be provided if Ingress is enabled\n    ##\n    hosts: []\n    #   - pushgateway.domain.com\n    #   - domain.com/pushgateway\n\n    path: /\n\n    # pathType is only for k8s \u003e= 1.18\n    pathType: Prefix\n\n    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.\n    extraPaths: []\n    # - path: /*\n    #   backend:\n    #     serviceName: ssl-redirect\n    #     servicePort: use-annotation\n\n    ## pushgateway Ingress TLS configuration\n    ## Secrets must be manually created in the namespace\n    ##\n    tls: []\n    #   - secretName: prometheus-alerts-tls\n    #     hosts:\n    #       - pushgateway.domain.com\n\n  ## Node tolerations for pushgateway scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal|Exists\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  ## Node labels for pushgateway pod assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Annotations to be added to pushgateway pods\n  ##\n  podAnnotations: {}\n\n  ## Labels to be added to pushgateway pods\n  ##\n  podLabels: {}\n\n  ## Specify if a Pod Security Policy for node-exporter must be created\n  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  ##\n  podSecurityPolicy:\n    annotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  replicaCount: 1\n\n  ## Annotations to be added to deployment\n  ##\n  deploymentAnnotations: {}\n\n  ## PodDisruptionBudget settings\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n  ##\n  podDisruptionBudget:\n    enabled: false\n    maxUnavailable: 1\n\n  ## pushgateway resource requests and limits\n  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n    # limits:\n    #   cpu: 10m\n    #   memory: 32Mi\n    # requests:\n    #   cpu: 10m\n    #   memory: 32Mi\n\n  ## Vertical Pod Autoscaler config\n  ## Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler\n  verticalAutoscaler:\n    ## If true a VPA object will be created for the controller\n    enabled: false\n    # updateMode: \"Auto\"\n    # containerPolicies:\n    # - containerName: 'prometheus-pushgateway'\n\n  # Custom DNS configuration to be added to push-gateway pods\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n\n  ## Security context to be added to push-gateway pods\n  ##\n  securityContext:\n    runAsUser: 65534\n    runAsNonRoot: true\n\n  service:\n    annotations:\n      prometheus.io/probe: pushgateway\n    labels: {}\n    clusterIP: \"\"\n\n    ## List of IP addresses at which the pushgateway service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 9091\n    type: ClusterIP\n\n  ## pushgateway Deployment Strategy type\n  # strategy:\n  #   type: Recreate\n\n  persistentVolume:\n    ## If true, pushgateway will create/use a Persistent Volume Claim\n    ##\n    enabled: false\n\n    ## pushgateway data Persistent Volume access modes\n    ## Must match those of existing PV or dynamic provisioner\n    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    ##\n    accessModes:\n      - ReadWriteOnce\n\n    ## pushgateway data Persistent Volume Claim annotations\n    ##\n    annotations: {}\n\n    ## pushgateway data Persistent Volume existing claim name\n    ## Requires pushgateway.persistentVolume.enabled: true\n    ## If defined, PVC must be created manually before volume will be bound\n    existingClaim: \"\"\n\n    ## pushgateway data Persistent Volume mount root path\n    ##\n    mountPath: /data\n\n    ## pushgateway data Persistent Volume size\n    ##\n    size: 2Gi\n\n    ## pushgateway data Persistent Volume Storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    # storageClass: \"-\"\n\n    ## pushgateway data Persistent Volume Binding Mode\n    ## If defined, volumeBindingMode: \u003cvolumeBindingMode\u003e\n    ## If undefined (the default) or set to null, no volumeBindingMode spec is\n    ##   set, choosing the default mode.\n    ##\n    # volumeBindingMode: \"\"\n\n    ## Subdirectory of pushgateway data Persistent Volume to mount\n    ## Useful if the volume's root directory is not empty\n    ##\n    subPath: \"\"\n\n\n## alertmanager ConfigMap entries\n##\nalertmanagerFiles:\n  alertmanager.yml:\n    global: {}\n      # slack_api_url: ''\n\n    receivers:\n      - name: default-receiver\n        # slack_configs:\n        #  - channel: '@you'\n        #    send_resolved: true\n\n    route:\n      group_wait: 10s\n      group_interval: 5m\n      receiver: default-receiver\n      repeat_interval: 3h\n\n## Prometheus server ConfigMap entries\n##\nserverFiles:\n\n  ## Alerts configuration\n  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/\n  alerting_rules.yml:\n    groups:\n    - name: demo alert\n      rules:\n      - alert: High Pod Memory\n        expr: sum(container_memory_usage_bytes) \u003e 10\n        for: 1m\n        labels:\n          severity: slack\n        annotations:\n          summary: High Memory Usage\n  # groups:\n  #   - name: Instances\n  #     rules:\n  #       - alert: InstanceDown\n  #         expr: up == 0\n  #         for: 5m\n  #         labels:\n  #           severity: page\n  #         annotations:\n  #           description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'\n  #           summary: 'Instance {{ $labels.instance }} down'\n  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use alerting_rules.yml\n  alerts: {}\n\n  ## Records configuration\n  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/\n  recording_rules.yml: {}\n  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use recording_rules.yml\n  rules: {}\n\n  prometheus.yml:\n    rule_files:\n      - /etc/config/recording_rules.yml\n      - /etc/config/alerting_rules.yml\n    ## Below two files are DEPRECATED will be removed from this default values file\n      - /etc/config/rules\n      - /etc/config/alerts\n\n    scrape_configs:\n      - job_name: prometheus\n        static_configs:\n          - targets:\n            - localhost:9090\n\n      # A scrape configuration for running Prometheus on a Kubernetes cluster.\n      # This uses separate scrape configs for cluster components (i.e. API server, node)\n      # and services to allow each to use different authentication configs.\n      #\n      # Kubernetes labels will be added as Prometheus labels on metrics via the\n      # `labelmap` relabeling action.\n\n      # Scrape config for API servers.\n      #\n      # Kubernetes exposes API servers as endpoints to the default/kubernetes\n      # service so this uses `endpoints` role and uses relabelling to only keep\n      # the endpoints associated with the default/kubernetes service using the\n      # default named port `https`. This works for single API server deployments as\n      # well as HA API server deployments.\n      - job_name: 'kubernetes-apiservers'\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        # Keep only the default/kubernetes service endpoints for the https port. This\n        # will add targets for each API server which Kubernetes adds an endpoint to\n        # the default/kubernetes service.\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n            action: keep\n            regex: default;kubernetes;https\n\n      - job_name: 'kubernetes-nodes'\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        kubernetes_sd_configs:\n          - role: node\n\n        relabel_configs:\n          - action: labelmap\n            regex: __meta_kubernetes_node_label_(.+)\n          - target_label: __address__\n            replacement: kubernetes.default.svc:443\n          - source_labels: [__meta_kubernetes_node_name]\n            regex: (.+)\n            target_label: __metrics_path__\n            replacement: /api/v1/nodes/$1/proxy/metrics\n\n\n      - job_name: 'kubernetes-nodes-cadvisor'\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        kubernetes_sd_configs:\n          - role: node\n\n        # This configuration will work only on kubelet 1.7.3+\n        # As the scrape endpoints for cAdvisor have changed\n        # if you are using older version you need to change the replacement to\n        # replacement: /api/v1/nodes/$1:4194/proxy/metrics\n        # more info here https://github.com/coreos/prometheus-operator/issues/633\n        relabel_configs:\n          - action: labelmap\n            regex: __meta_kubernetes_node_label_(.+)\n          - target_label: __address__\n            replacement: kubernetes.default.svc:443\n          - source_labels: [__meta_kubernetes_node_name]\n            regex: (.+)\n            target_label: __metrics_path__\n            replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor\n\n      # Scrape config for service endpoints.\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/scrape`: Only scrape services that have a value of\n      # `true`, except if `prometheus.io/scrape-slow` is set to `true` as well.\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: If the metrics are exposed on a different port to the\n      # service then set this appropriately.\n      # * `prometheus.io/param_\u003cparameter\u003e`: If the metrics endpoint uses parameters\n      # then you can set any parameter\n      - job_name: 'kubernetes-service-endpoints'\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]\n            action: drop\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n            action: replace\n            target_label: __scheme__\n            regex: (https?)\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n            action: replace\n            target_label: __address__\n            regex: ([^:]+)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n          - action: labelmap\n            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            action: replace\n            target_label: service\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n      # Scrape config for slow service endpoints; same as above, but with a larger\n      # timeout and a larger interval\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: If the metrics are exposed on a different port to the\n      # service then set this appropriately.\n      # * `prometheus.io/param_\u003cparameter\u003e`: If the metrics endpoint uses parameters\n      # then you can set any parameter\n      - job_name: 'kubernetes-service-endpoints-slow'\n\n        scrape_interval: 5m\n        scrape_timeout: 30s\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n            action: replace\n            target_label: __scheme__\n            regex: (https?)\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n            action: replace\n            target_label: __address__\n            regex: ([^:]+)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n          - action: labelmap\n            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            action: replace\n            target_label: service\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n      - job_name: 'prometheus-pushgateway'\n        honor_labels: true\n\n        kubernetes_sd_configs:\n          - role: service\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n            action: keep\n            regex: pushgateway\n\n      # Example scrape config for probing services via the Blackbox Exporter.\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/probe`: Only probe services that have a value of `true`\n      - job_name: 'kubernetes-services'\n\n        metrics_path: /probe\n        params:\n          module: [http_2xx]\n\n        kubernetes_sd_configs:\n          - role: service\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n            action: keep\n            regex: true\n          - source_labels: [__address__]\n            target_label: __param_target\n          - target_label: __address__\n            replacement: blackbox\n          - source_labels: [__param_target]\n            target_label: instance\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            target_label: service\n\n      # Example scrape config for pods\n      #\n      # The relabeling allows the actual pod scrape endpoint to be configured via the\n      # following annotations:\n      #\n      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`,\n      # except if `prometheus.io/scrape-slow` is set to `true` as well.\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.\n      - job_name: 'kubernetes-pods'\n\n        kubernetes_sd_configs:\n          - role: pod\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]\n            action: drop\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]\n            action: replace\n            regex: (https?)\n            target_label: __scheme__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n            action: replace\n            regex: ([^:]+)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n            target_label: __address__\n          - action: labelmap\n            regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_pod_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_pod_name]\n            action: replace\n            target_label: pod\n          - source_labels: [__meta_kubernetes_pod_phase]\n            regex: Pending|Succeeded|Failed|Completed\n            action: drop\n\n      # Example Scrape config for pods which should be scraped slower. An useful example\n      # would be stackriver-exporter which queries an API on every scrape of the pod\n      #\n      # The relabeling allows the actual pod scrape endpoint to be configured via the\n      # following annotations:\n      #\n      # * `prometheus.io/scrape-slow`: Only scrape pods that have a value of `true`\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.\n      - job_name: 'kubernetes-pods-slow'\n\n        scrape_interval: 5m\n        scrape_timeout: 30s\n\n        kubernetes_sd_configs:\n          - role: pod\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]\n            action: replace\n            regex: (https?)\n            target_label: __scheme__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n            action: replace\n            regex: ([^:]+)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n            target_label: __address__\n          - action: labelmap\n            regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_pod_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_pod_name]\n            action: replace\n            target_label: pod\n          - source_labels: [__meta_kubernetes_pod_phase]\n            regex: Pending|Succeeded|Failed|Completed\n            action: drop\n\n# adds additional scrape configs to prometheus.yml\n# must be a string so you have to add a | after extraScrapeConfigs:\n# example adds prometheus-blackbox-exporter scrape config\nextraScrapeConfigs:\n  # - job_name: 'prometheus-blackbox-exporter'\n  #   metrics_path: /probe\n  #   params:\n  #     module: [http_2xx]\n  #   static_configs:\n  #     - targets:\n  #       - https://example.com\n  #   relabel_configs:\n  #     - source_labels: [__address__]\n  #       target_label: __param_target\n  #     - source_labels: [__param_target]\n  #       target_label: instance\n  #     - target_label: __address__\n  #       replacement: prometheus-blackbox-exporter:9115\n\n# Adds option to add alert_relabel_configs to avoid duplicate alerts in alertmanager\n# useful in H/A prometheus with different external labels but the same alerts\nalertRelabelConfigs:\n  # alert_relabel_configs:\n  # - source_labels: [dc]\n  #   regex: (.+)\\d+\n  #   target_label: dc\n\nnetworkPolicy:\n  ## Enable creation of NetworkPolicy resources.\n  ##\n  enabled: false\n\n# Force namespace of namespaced resources\nforceNamespace: null\n"
            ],
            "verify": false,
            "version": "15.1.3",
            "wait": true
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "data.template_file.prometheus_values"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "vault",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "vault",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "vault",
            "keyring": null,
            "lint": false,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.7.2",
                "chart": "vault",
                "name": "vault",
                "namespace": "staging",
                "revision": 1,
                "values": "{\"server\":{\"ha\":{\"enabled\":true,\"replicas\":1}}}",
                "version": "0.12.0"
              }
            ],
            "name": "vault",
            "namespace": "staging",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://helm.releases.hashicorp.com",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "server.ha.enabled",
                "type": "",
                "value": "true"
              },
              {
                "name": "server.ha.replicas",
                "type": "",
                "value": "1"
              }
            ],
            "set_sensitive": [],
            "set_string": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": null,
            "verify": false,
            "version": "0.12.0",
            "wait": true
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "helm_release.consul",
            "kubernetes_namespace.this",
            "kubernetes_secret.gossip_key",
            "random_id.gossip_key"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "this",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "staging",
            "metadata": [
              {
                "annotations": null,
                "generate_name": "",
                "generation": 0,
                "labels": null,
                "name": "staging",
                "resource_version": "72924",
                "self_link": "",
                "uid": "7af8d4f7-6b24-4540-aab8-7ad940fe7d35"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_secret",
      "name": "gossip_key",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "data": {
              "gossip.key": "qnFMdTHBa57o275h6lZcLBVepqs/3cfq41kXRjrJqWA="
            },
            "id": "staging/gossip-key",
            "metadata": [
              {
                "annotations": null,
                "generate_name": "",
                "generation": 0,
                "labels": null,
                "name": "gossip-key",
                "namespace": "staging",
                "resource_version": "72929",
                "self_link": "",
                "uid": "44cec9dc-5d38-4063-b6a4-dad4ff8def56"
              }
            ],
            "type": "Opaque"
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "kubernetes_namespace.this",
            "random_id.gossip_key"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "random_id",
      "name": "gossip_key",
      "provider": "provider[\"registry.terraform.io/hashicorp/random\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "b64_std": "qnFMdTHBa57o275h6lZcLBVepqs/3cfq41kXRjrJqWA=",
            "b64_url": "qnFMdTHBa57o275h6lZcLBVepqs_3cfq41kXRjrJqWA",
            "byte_length": 32,
            "dec": "77093365669751230418693251821141975757870108414595137122927203783249379305824",
            "hex": "aa714c7531c16b9ee8dbbe61ea565c2c155ea6ab3fddc7eae35917463ac9a960",
            "id": "qnFMdTHBa57o275h6lZcLBVepqs_3cfq41kXRjrJqWA",
            "keepers": null,
            "prefix": null
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    }
  ]
}
